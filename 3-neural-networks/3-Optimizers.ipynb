{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD(stochastic gradient descent)\n",
    "\n",
    "For model parameters $\\theta$, averaged parameter gradients $\\nabla_{\\theta} \\mathcal{L}$, and learning rate $\\eta$, the SGD update at timestep $t$ is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "            \\text{update}^{(t)}\n",
    "                &=  \\text{momentum} \\cdot \\text{update}^{(t-1)} + \\eta^{(t)} \\nabla_{\\theta} \\mathcal{L}\\\\\n",
    "            \\theta^{(t+1)}\n",
    "                &\\leftarrow  \\theta^{(t)} - \\text{update}^{(t)}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(\n",
    "        self, lr=0.01, momentum=0.0, clip_norm=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            Learning rate for SGD. If scheduler is not None, this is used as\n",
    "            the starting learning rate. Default is 0.01.\n",
    "        momentum : float in range [0, 1]\n",
    "            The fraction of the previous update to add to the current update.\n",
    "            If 0, no momentum is applied. Default is 0.\n",
    "        clip_norm : float\n",
    "            If not None, all param gradients are scaled to have maximum l2 norm of\n",
    "            `clip_norm` before computing update. Default is None.\n",
    "        \"\"\"\n",
    "        self.hyperparameters = {\n",
    "            \"id\": \"SGD\",\n",
    "            \"lr\": lr,\n",
    "            \"momentum\": momentum,\n",
    "            \"clip_norm\": clip_norm,\n",
    "        }\n",
    "\n",
    "    def update(self, param, param_grad, param_name, cur_loss=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        param : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of the parameter to be updated.\n",
    "        param_grad : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The gradient of the loss function with respect to `param_name`.\n",
    "        param_name : str\n",
    "            The name of the parameter.\n",
    "        cur_loss : float\n",
    "            The training or validation loss for the current minibatch. Used for\n",
    "            learning rate scheduling e.g., by\n",
    "            :class:`~numpy_ml.neural_nets.schedulers.KingScheduler`.\n",
    "            Default is None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        updated_params : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of `param` after applying the momentum update.\n",
    "        \"\"\"\n",
    "        C = self.cache\n",
    "        H = self.hyperparameters\n",
    "        momentum, clip_norm = H[\"momentum\"], H[\"clip_norm\"]\n",
    "        lr = H[\"lr\"]\n",
    "\n",
    "        if param_name not in C:\n",
    "            C[param_name] = np.zeros_like(param_grad)\n",
    "\n",
    "        # scale gradient to avoid explosion\n",
    "        t = np.inf if clip_norm is None else clip_norm\n",
    "        if norm(param_grad) > t:\n",
    "            param_grad = param_grad * t / norm(param_grad)\n",
    "\n",
    "        update = momentum * C[param_name] + lr * param_grad\n",
    "        self.cache[param_name] = update\n",
    "        return param - update\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "weights that receive large gradients will have their effective learning rate reduced, while weights that receive small or infrequent updates will have their effective learning rate increased.\n",
    "\n",
    "$$\n",
    "  \\begin{align*}\n",
    "  \\text{cache}^{(t)} &= \\text{cache}^{(t-1)} + (\\nabla_{\\theta} \\mathcal{L})^2\\\\\n",
    "  \\text{update}^{(t)} &= \\alpha \\frac{\\nabla_{\\theta} \\mathcal{L}}{\\sqrt{\\text{cache}^{(t)}} + \\varepsilon} \\\\\n",
    "  \\theta^{(t+1)} &= \\theta^{(t)} - \\text{update}^{(t)} \\\\\n",
    "  \\end{align*}\n",
    "$$\n",
    "\n",
    "Note that the ``**`` and `/` operations are elementwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad():\n",
    "    def __init__(self, lr=0.01, eps=1e-7, clip_norm=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            Global learning rate\n",
    "        eps : float\n",
    "            Smoothing term to avoid divide-by-zero errors in the update calc.\n",
    "            Default is 1e-7.\n",
    "        clip_norm : float or None\n",
    "            If not None, all param gradients are scaled to have maximum `L2` norm of\n",
    "            `clip_norm` before computing update. Default is None.\n",
    "        \"\"\"\n",
    "\n",
    "        self.cache = {}\n",
    "        self.hyperparameters = {\n",
    "            \"id\": \"AdaGrad\",\n",
    "            \"lr\": lr,\n",
    "            \"eps\": eps,\n",
    "            \"clip_norm\": clip_norm,\n",
    "        }\n",
    "\n",
    "    def update(self, param, param_grad, param_name, cur_loss=None):\n",
    "        \"\"\"\n",
    "        Compute the AdaGrad update for a given parameter.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Adjusts the learning rate of each weight based on the magnitudes of its\n",
    "        gradients (big gradient -> small lr, small gradient -> big lr).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        param : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of the parameter to be updated\n",
    "        param_grad : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The gradient of the loss function with respect to `param_name`\n",
    "        param_name : str\n",
    "            The name of the parameter\n",
    "        cur_loss : float or None\n",
    "            The training or validation loss for the current minibatch. Used for\n",
    "            learning rate scheduling e.g., by\n",
    "            :class:`~numpy_ml.neural_nets.schedulers.KingScheduler`.\n",
    "            Default is None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        updated_params : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of `param` after applying the AdaGrad update\n",
    "        \"\"\"\n",
    "        C = self.cache\n",
    "        H = self.hyperparameters\n",
    "        eps, clip_norm = H[\"eps\"], H[\"clip_norm\"]\n",
    "        lr = H[\"lr\"]\n",
    "\n",
    "        if param_name not in C:\n",
    "            C[param_name] = np.zeros_like(param_grad)\n",
    "\n",
    "        # scale gradient to avoid explosion\n",
    "        t = np.inf if clip_norm is None else clip_norm\n",
    "        if norm(param_grad) > t:\n",
    "            param_grad = param_grad * t / norm(param_grad)\n",
    "\n",
    "        C[param_name] += param_grad ** 2\n",
    "        update = lr * param_grad / (np.sqrt(C[param_name]) + eps)\n",
    "        self.cache = C\n",
    "        return param - update\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "RMSProp was proposed as a refinement of :class:`AdaGrad` to reduce its aggressive, monotonically decreasing learning rate.\n",
    "\n",
    "RMSProp uses a *decaying average* of the previous squared gradients (second moment) rather than just the immediately preceding squared gradient for its `previous_update` value.\n",
    "\n",
    "$$\n",
    "  \\begin{align*}\n",
    "  \\text{cache}^{(t)} &= \\beta \\text{cache}^{(t-1)} + (1 - \\beta)(\\nabla_{\\theta} \\mathcal{L})^2\\\\\n",
    "  \\text{update}^{(t)} &= \\alpha \\frac{\\nabla_{\\theta} \\mathcal{L}}{\\sqrt{\\text{cache}^{(t)}} + \\varepsilon} \\\\\n",
    "  \\theta^{(t+1)} &= \\theta^{(t)} - \\text{update}^{(t)} \\\\\n",
    "  \\end{align*}\n",
    "$$\n",
    "\n",
    "Note that the ``**`` and ``/`` operations are elementwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp():\n",
    "    def __init__(\n",
    "        self, lr=0.001, decay=0.9, eps=1e-7, clip_norm=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            Learning rate for update. Default is 0.001.\n",
    "        decay : float in [0, 1]\n",
    "            Rate of decay for the moving average. Typical values are [0.9,\n",
    "            0.99, 0.999]. Default is 0.9.\n",
    "        eps : float\n",
    "            Constant term to avoid divide-by-zero errors during the update calc. Default is 1e-7.\n",
    "        clip_norm : float or None\n",
    "            If not None, all param gradients are scaled to have maximum l2 norm of\n",
    "            `clip_norm` before computing update. Default is None.\n",
    "        \"\"\"\n",
    "        self.cache = {}\n",
    "        self.hyperparameters = {\n",
    "            \"id\": \"RMSProp\",\n",
    "            \"lr\": lr,\n",
    "            \"eps\": eps,\n",
    "            \"decay\": decay,\n",
    "            \"clip_norm\": clip_norm,\n",
    "        }\n",
    "\n",
    "    def update(self, param, param_grad, param_name, cur_loss=None):\n",
    "        \"\"\"\n",
    "        Compute the RMSProp update for a given parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        param : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of the parameter to be updated\n",
    "        param_grad : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The gradient of the loss function with respect to `param_name`\n",
    "        param_name : str\n",
    "            The name of the parameter\n",
    "        cur_loss : float or None\n",
    "            The training or validation loss for the current minibatch. Used for\n",
    "            learning rate scheduling e.g., by\n",
    "            :class:`~numpy_ml.neural_nets.schedulers.KingScheduler`.\n",
    "            Default is None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        updated_params : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of `param` after applying the RMSProp update.\n",
    "        \"\"\"\n",
    "        C = self.cache\n",
    "        H = self.hyperparameters\n",
    "        eps, decay, clip_norm = H[\"eps\"], H[\"decay\"], H[\"clip_norm\"]\n",
    "        lr = H[\"lr\"] \n",
    "\n",
    "        if param_name not in C:\n",
    "            C[param_name] = np.zeros_like(param_grad)\n",
    "\n",
    "        # scale gradient to avoid explosion\n",
    "        t = np.inf if clip_norm is None else clip_norm\n",
    "        if norm(param_grad) > t:\n",
    "            param_grad = param_grad * t / norm(param_grad)\n",
    "\n",
    "        C[param_name] = decay * C[param_name] + (1 - decay) * param_grad ** 2\n",
    "        update = lr * param_grad / (np.sqrt(C[param_name]) + eps)\n",
    "        self.cache = C\n",
    "        return param - update\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam(adaptive moment estimation)\n",
    "\n",
    "Designed to combine the advantages of :class:`AdaGrad`, which works well with sparse gradients, and :class:`RMSProp`, which works well in online and non-stationary settings.\n",
    "\n",
    "$$\n",
    "  \\begin{align*}\n",
    "  m^t &= \\beta_1 m^{(t-1)} + (1 - \\beta_1)\\nabla_{\\theta} \\mathcal{L}\\\\\n",
    "  v^t &= \\beta_2 v^{(t-1)} + (1 - \\beta_2)(\\nabla_{\\theta} \\mathcal{L})^2\\\\\n",
    "  \\hat m &= \\frac{m^t}{1 - (\\beta_1)^t} \\\\\n",
    "  \\hat v &= \\frac{v^t}{1 - (\\beta_2)^t}\\\\\n",
    "  \\text{update}^{(t-1)} &= \\alpha \\frac{\\hat v}{\\sqrt{\\hat m} + \\varepsilon} \\\\\n",
    "  \\theta^{(t+1)} &= \\theta^{(t)} - \\text{update}^{(t-1)} \\\\\n",
    "  \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr=0.001,\n",
    "        decay1=0.9,\n",
    "        decay2=0.999,\n",
    "        eps=1e-7,\n",
    "        clip_norm=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            Learning rate for update. This parameter is ignored if using\n",
    "            :class:`~numpy_ml.neural_nets.schedulers.NoamScheduler`.\n",
    "            Default is 0.001.\n",
    "        decay1 : float\n",
    "            The rate of decay to use for in running estimate of the first\n",
    "            moment (mean) of the gradient. Default is 0.9.\n",
    "        decay2 : float\n",
    "            The rate of decay to use for in running estimate of the second\n",
    "            moment (variance) of the gradient. Default is 0.999.\n",
    "        eps : float\n",
    "            Constant term to avoid divide-by-zero errors during the update\n",
    "            calc. Default is 1e-7.\n",
    "        clip_norm : float\n",
    "            If not None, all param gradients are scaled to have maximum l2 norm of\n",
    "            `clip_norm` before computing update. Default is None.\n",
    "        \"\"\"\n",
    "\n",
    "        self.cache = {}\n",
    "        self.hyperparameters = {\n",
    "            \"id\": \"Adam\",\n",
    "            \"lr\": lr,\n",
    "            \"eps\": eps,\n",
    "            \"decay1\": decay1,\n",
    "            \"decay2\": decay2,\n",
    "            \"clip_norm\": clip_norm,\n",
    "        }\n",
    "\n",
    "    def update(self, param, param_grad, param_name, cur_loss=None):\n",
    "        \"\"\"\n",
    "        Compute the Adam update for a given parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        param : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of the parameter to be updated.\n",
    "        param_grad : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The gradient of the loss function with respect to `param_name`.\n",
    "        param_name : str\n",
    "            The name of the parameter.\n",
    "        cur_loss : float\n",
    "            The training or validation loss for the current minibatch. Used for\n",
    "            learning rate scheduling e.g., by\n",
    "            :class:`~numpy_ml.neural_nets.schedulers.KingScheduler`. Default is\n",
    "            None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        updated_params : :py:class:`ndarray <numpy.ndarray>` of shape (n, m)\n",
    "            The value of `param` after applying the Adam update.\n",
    "        \"\"\"\n",
    "        C = self.cache\n",
    "        H = self.hyperparameters\n",
    "        d1, d2 = H[\"decay1\"], H[\"decay2\"]\n",
    "        eps, clip_norm = H[\"eps\"], H[\"clip_norm\"]\n",
    "        lr = H[\"lr\"]\n",
    "\n",
    "        if param_name not in C:\n",
    "            C[param_name] = {\n",
    "                \"t\": 0,\n",
    "                \"mean\": np.zeros_like(param_grad),\n",
    "                \"var\": np.zeros_like(param_grad),\n",
    "            }\n",
    "\n",
    "        # scale gradient to avoid explosion\n",
    "        t = np.inf if clip_norm is None else clip_norm\n",
    "        if norm(param_grad) > t:\n",
    "            param_grad = param_grad * t / norm(param_grad)\n",
    "\n",
    "        t = C[param_name][\"t\"] + 1\n",
    "        var = C[param_name][\"var\"]\n",
    "        mean = C[param_name][\"mean\"]\n",
    "\n",
    "        # update cache\n",
    "        C[param_name][\"t\"] = t\n",
    "        C[param_name][\"var\"] = d2 * var + (1 - d2) * param_grad ** 2\n",
    "        C[param_name][\"mean\"] = d1 * mean + (1 - d1) * param_grad\n",
    "        self.cache = C\n",
    "\n",
    "        # calc unbiased moment estimates and Adam update\n",
    "        v_hat = C[param_name][\"var\"] / (1 - d2 ** t)\n",
    "        m_hat = C[param_name][\"mean\"] / (1 - d1 ** t)\n",
    "        update = lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "        return param - update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyter)",
   "language": "python",
   "name": "pycharm-34f9d306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
