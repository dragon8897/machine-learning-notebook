{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SquaredError\n",
    "\n",
    "$$\n",
    "                \\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}})\n",
    "                    = \\frac12 ||\\hat{\\mathbf{y}} - \\mathbf{y}||_2^2\n",
    "$$\n",
    "\n",
    "用于: 实数 **y**\n",
    "\n",
    "1 阶导数:\n",
    "$$\n",
    "            \\mathcal{L}(\\mathbf{z})\n",
    "                =  \\text{squared_error}(\\mathbf{y}, g(\\mathbf{z})) \\\\\n",
    "            g(\\mathbf{z})\n",
    "                =  \\text{act_fn}(\\mathbf{z}) \\\\\n",
    "            \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}}\n",
    "                = (g(\\mathbf{z}) - \\mathbf{y}) \\left(\n",
    "                    \\frac{\\partial g}{\\partial \\mathbf{z}} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredError():\n",
    "    def loss(y, y_pred):\n",
    "        return 0.5 * np.linalg.norm(y_pred - y) ** 2\n",
    "\n",
    "    def grad(y, y_pred, z, act_fn):\n",
    "        return (y_pred - y) * act_fn.grad(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEntropy\n",
    "\n",
    "$$\n",
    "                \\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}})\n",
    "                    = \\sum_i y_i \\log \\hat{y}_i\n",
    "$$\n",
    "\n",
    "用于: one-hot **y**\n",
    "\n",
    "预测: 分类概率\n",
    "\n",
    "1 阶导数:\n",
    "$$\n",
    "            \\mathcal{L}(\\mathbf{z})\n",
    "                = \\text{cross_entropy}(\\text{softmax}(\\mathbf{z})) \\\\\n",
    "            \\begin{align*}\n",
    "            \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}}\n",
    "                &= \\text{softmax}(\\mathbf{z}) - \\mathbf{y} \\\\\n",
    "                &=  \\hat{\\mathbf{y}} - \\mathbf{y}\n",
    "            \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy():\n",
    "    def loss(y, y_pred):\n",
    "        # prevent taking the log of 0\n",
    "        eps = np.finfo(float).eps\n",
    "\n",
    "        # each example is associated with a single class; sum the negative log\n",
    "        # probability of the correct label over all samples in the batch.\n",
    "        # observe that we are taking advantage of the fact that y is one-hot\n",
    "        # encoded\n",
    "        cross_entropy = -np.sum(y * np.log(y_pred + eps))\n",
    "        return cross_entropy\n",
    "\n",
    "    def grad(y, y_pred):\n",
    "        # derivative of xe wrt z is y_pred - y_true, hence we can just\n",
    "        # subtract 1 from the probability of the correct class labels\n",
    "        grad = y_pred - y\n",
    "\n",
    "        # [optional] scale the gradients by the number of examples in the batch\n",
    "        # n, m = y.shape\n",
    "        # grad /= n\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAELoss (The variational lower bound for a variational autoencoder with Bernoulli units.)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "            \\text{VAELoss} =\n",
    "                \\text{cross_entropy}(\\mathbf{y}, \\hat{\\mathbf{y}})\n",
    "                    + \\mathbb{KL}[q \\ || \\ p]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The VLB to the sum of the binary cross entropy between the true input and the predicted output (the \"reconstruction loss\") and the KL divergence between the learned variational distribution :math:`q` and the prior, :math:`p`, assumed to be a unit Gaussian.\n",
    "\n",
    "- $\\mathbb{KL}[q \\ || \\ p]$: Kullback-Leibler divergence between the distributions $q$ and $p$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELoss():\n",
    "    def loss(y, y_pred, t_mean, t_log_var):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, N)`\n",
    "            The original images.\n",
    "        y_pred : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, N)`\n",
    "            The VAE reconstruction of the images.\n",
    "        t_mean: :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, T)`\n",
    "            Mean of the variational distribution :math:`q(t \\mid x)`.\n",
    "        t_log_var: :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, T)`\n",
    "            Log of the variance vector of the variational distribution\n",
    "            :math:`q(t \\mid x)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            The VLB, averaged across the batch.\n",
    "        \"\"\"\n",
    "        # prevent nan on log(0)\n",
    "        eps = np.finfo(float).eps\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "        # reconstruction loss: binary cross-entropy\n",
    "        rec_loss = -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred), axis=1)\n",
    "\n",
    "        # KL divergence between the variational distribution q and the prior p,\n",
    "        # a unit gaussian\n",
    "        kl_loss = -0.5 * np.sum(1 + t_log_var - t_mean ** 2 - np.exp(t_log_var), axis=1)\n",
    "        loss = np.mean(kl_loss + rec_loss)\n",
    "        return loss\n",
    "\n",
    "    def grad(y, y_pred, t_mean, t_log_var):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, N)`\n",
    "            The original images.\n",
    "        y_pred : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, N)`\n",
    "            The VAE reconstruction of the images.\n",
    "        t_mean: :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, T)`\n",
    "            Mean of the variational distribution :math:`q(t | x)`.\n",
    "        t_log_var: :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, T)`\n",
    "            Log of the variance vector of the variational distribution\n",
    "            :math:`q(t | x)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dY_pred : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, N)`\n",
    "            The gradient of the VLB with regard to `y_pred`.\n",
    "        dLogVar : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, T)`\n",
    "            The gradient of the VLB with regard to `t_log_var`.\n",
    "        dMean : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, T)`\n",
    "            The gradient of the VLB with regard to `t_mean`.\n",
    "        \"\"\"\n",
    "        N = y.shape[0]\n",
    "        eps = np.finfo(float).eps\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "        dY_pred = -y / (N * y_pred) - (y - 1) / (N - N * y_pred)\n",
    "        dLogVar = (np.exp(t_log_var) - 1) / (2 * N)\n",
    "        dMean = t_mean / N\n",
    "        return dY_pred, dLogVar, dMean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGAN_GPLoss\n",
    "\n",
    "The loss function for a Wasserstein GAN with gradient penalty.\n",
    "\n",
    "Assuming an optimal critic, minimizing this quantity wrt. the generator parameters corresponds to minimizing the Wasserstein-1 (earth-mover) distance between the fake and real data distributions.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "            \\text{WGANLoss}\n",
    "                &=  \\sum_{x \\in X_{real}} p(x) D(x)\n",
    "                    - \\sum_{x' \\in X_{fake}} p(x') D(x') \\\\\n",
    "            \\text{WGANLossGP}\n",
    "                &=  \\text{WGANLoss} + \\lambda\n",
    "                    (||\\nabla_{X_{interp}} D(X_{interp})||_2 - 1)^2 \\\\\n",
    "            X_{fake}  &=   \\text{Generator}(\\mathbf{z}) \\\\\n",
    "            X_{interp}   &=   \\alpha X_{real} + (1 - \\alpha) X_{fake} \\\\\n",
    "            \\mathbf{z}  &\\sim  \\mathcal{N}(0, \\mathbb{1}) \\\\\n",
    "            \\alpha  &\\sim  \\text{Uniform}(0, 1)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN_GPLoss():\n",
    "    def __init__(self, lambda_=10):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        lambda_ : float\n",
    "            The gradient penalty coefficient. Default is 10.\n",
    "        \"\"\"\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def loss(self, Y_fake, module, Y_real=None, gradInterp=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y_fake : :py:class:`ndarray <numpy.ndarray>` of shape (n_ex,)\n",
    "            The output of the critic for `X_fake`.\n",
    "        module : {'C', 'G'}\n",
    "            Whether to calculate the loss for the critic ('C') or the generator\n",
    "            ('G'). If calculating loss for the critic, `Y_real` and\n",
    "            `gradInterp` must not be None.\n",
    "        Y_real : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex,)`, or\n",
    "        None\n",
    "            The output of the critic for `X_real`. Default is None.\n",
    "        gradInterp : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex,\n",
    "        n_feats)` or None\n",
    "            The gradient of the critic output for `X_interp` wrt. `X_interp`.\n",
    "            Default is None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            Depending on the setting for `module`, either the critic or\n",
    "            generator loss, averaged over examples in the minibatch.\n",
    "        \"\"\"\n",
    "        # calc critic loss including gradient penalty\n",
    "        if module == \"C\":\n",
    "            X_interp_norm = np.linalg.norm(gradInterp, axis=1, keepdims=True)\n",
    "            gradient_penalty = (X_interp_norm - 1) ** 2\n",
    "            loss = (\n",
    "                Y_fake.mean() - Y_real.mean() + self.lambda_ * gradient_penalty.mean()\n",
    "            )\n",
    "\n",
    "        # calc generator loss\n",
    "        elif module == \"G\":\n",
    "            loss = -Y_fake.mean()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized module: {}\".format(module))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def grad(self, Y_fake, module, Y_real=None, gradInterp=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y_fake : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex,)`\n",
    "            The output of the critic for `X_fake`.\n",
    "        module : {'C', 'G'}\n",
    "            Whether to calculate the gradient for the critic loss ('C') or the\n",
    "            generator loss ('G'). If calculating grads for the critic, `Y_real`\n",
    "            and `gradInterp` must not be None.\n",
    "        Y_real : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex,)`, or\n",
    "        None\n",
    "            The output of the critic for `X_real`. Default is None.\n",
    "        gradInterp : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex,\n",
    "        n_feats)`, or None\n",
    "            The gradient of the critic output on `X_interp` wrt. `X_interp`.\n",
    "            Default is None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grads : tuple\n",
    "            If `module` == 'C', returns a 3-tuple containing the gradient of\n",
    "            the critic loss with regard to (`Y_fake`, `Y_real`, `gradInterp`).\n",
    "            If `module` == 'G', returns the gradient of the generator with\n",
    "            regard to `Y_fake`.\n",
    "        \"\"\"\n",
    "        eps = np.finfo(float).eps\n",
    "        n_ex_fake = Y_fake.shape[0]\n",
    "\n",
    "        # calc gradient of the critic loss\n",
    "        if module == \"C\":\n",
    "            n_ex_real = Y_real.shape[0]\n",
    "\n",
    "            dY_fake = -1 / n_ex_fake * np.ones_like(Y_fake)\n",
    "            dY_real = 1 / n_ex_real * np.ones_like(Y_real)\n",
    "\n",
    "            # differentiate through gradient penalty\n",
    "            X_interp_norm = np.linalg.norm(gradInterp, axis=1, keepdims=True) + eps\n",
    "\n",
    "            dGradInterp = (\n",
    "                (2 / n_ex_fake)\n",
    "                * self.lambda_\n",
    "                * (X_interp_norm - 1)\n",
    "                * (gradInterp / X_interp_norm)\n",
    "            )\n",
    "            grad = (dY_fake, dY_real, dGradInterp)\n",
    "\n",
    "        # calc gradient of the generator loss\n",
    "        elif module == \"G\":\n",
    "            grad = -1 / n_ex_fake * np.ones_like(Y_fake)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized module: {}\".format(module))\n",
    "        return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCELoss\n",
    "\n",
    "A noise contrastive estimation (NCE) loss function.\n",
    "\n",
    "noise contrastive estimation is a candidate sampling method often used to reduce the computational challenge of training a softmax layer on problems with a large number of output classes. it proceeds by training a logistic regression model to discriminate between samples from the true data distribution and samples from an artificial noise distribution. \n",
    "\n",
    "It can be shown that as the ratio of negative samples to data samples goes to infinity, the gradient of the nce loss converges to the original softmax gradient. \n",
    "\n",
    "For input data **x** , target labels `targets`, loss parameters **w** and **b**, and noise samples `noise` sampled from the noise distribution `Q`, the NCE loss is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "            \\text{NCE}(X, targets) &=\n",
    "                \\text{cross_entropy}(\\mathbf{y}_{targets}, \\hat{\\mathbf{y}}_{targets}) +\n",
    "                \\text{cross_entropy}(\\mathbf{y}_{noise}, \\hat{\\mathbf{y}}_{noise}) \\\\\n",
    "            \\hat{\\mathbf{y}}_{targets}\n",
    "                &=  \\sigma(\\mathbf{W}[targets] \\mathbf{X} + \\mathbf{b}[targets] - \\log Q(targets)) \\\\\n",
    "            \\hat{\\mathbf{y}}_{noise}\n",
    "                &=  \\sigma(\\mathbf{W}[noise] \\mathbf{X} + \\mathbf{b}[noise] - \\log Q(noise))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In the above equations, $\\sigma$ is the logistic sigmoid function, and $Q(x)$ corresponds to the probability of the values in $x$ under $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCELoss():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes,\n",
    "        noise_sampler,\n",
    "        num_negative_samples,\n",
    "        optimizer=None,\n",
    "        init=\"glorot_uniform\",\n",
    "        subtract_log_label_prob=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_classes : int\n",
    "            The total number of output classes in the model.\n",
    "        noise_sampler : :class:`~numpy_ml.utils.data_structures.DiscreteSampler` instance\n",
    "            The negative sampler. Defines a distribution over all classes in\n",
    "            the dataset.\n",
    "        num_negative_samples : int\n",
    "            The number of negative samples to draw for each target / batch of\n",
    "            targets.\n",
    "        init : {'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'}\n",
    "            The weight initialization strategy. Default is 'glorot_uniform'.\n",
    "        optimizer : str, :doc:`Optimizer <numpy_ml.neural_nets.optimizers>` object, or None\n",
    "            The optimization strategy to use when performing gradient updates\n",
    "            within the :meth:`update` method.  If None, use the :class:`SGD\n",
    "            <numpy_ml.neural_nets.optimizers.optimizers.SGD>` optimizer with\n",
    "            default parameters. Default is None.\n",
    "        subtract_log_label_prob : bool\n",
    "            Whether to subtract the log of the probability of each label under\n",
    "            the noise distribution from its respective logit. Set to False for\n",
    "            negative sampling, True for NCE. Default is True.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        gradients : dict\n",
    "            The accumulated parameter gradients.\n",
    "        parameters: dict\n",
    "            The loss parameter values.\n",
    "        hyperparameters: dict\n",
    "            The loss hyperparameter values.\n",
    "        derived_variables: dict\n",
    "            Useful intermediate values computed during the loss computation.\n",
    "\n",
    "        \"\"\"\n",
    "        self.init = init\n",
    "        self.n_in = None\n",
    "        self.n_classes = n_classes\n",
    "        self.noise_sampler = noise_sampler\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        self.act_fn = ActivationInitializer(\"Sigmoid\")()\n",
    "        self.optimizer = OptimizerInitializer(optimizer)()\n",
    "        self.subtract_log_label_prob = subtract_log_label_prob\n",
    "\n",
    "        self.is_initialized = False\n",
    "\n",
    "    def _init_params(self):\n",
    "        init_weights = WeightInitializer(str(self.act_fn), mode=self.init)\n",
    "\n",
    "        self.X = []\n",
    "        b = np.zeros((1, self.n_classes))\n",
    "        W = init_weights((self.n_classes, self.n_in))\n",
    "\n",
    "        self.parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "        self.gradients = {\"W\": np.zeros_like(W), \"b\": np.zeros_like(b)}\n",
    "\n",
    "        self.derived_variables = {\n",
    "            \"y_pred\": [],\n",
    "            \"target\": [],\n",
    "            \"true_w\": [],\n",
    "            \"true_b\": [],\n",
    "            \"sampled_b\": [],\n",
    "            \"sampled_w\": [],\n",
    "            \"out_labels\": [],\n",
    "            \"target_logits\": [],\n",
    "            \"noise_samples\": [],\n",
    "            \"noise_logits\": [],\n",
    "        }\n",
    "\n",
    "        self.is_initialized = True\n",
    "\n",
    "    def loss(self, X, target, neg_samples=None, retain_derived=True):\n",
    "        \"\"\"\n",
    "        Compute the NCE loss for a collection of inputs and associated targets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, n_c, n_in)`\n",
    "            Layer input. A minibatch of `n_ex` examples, where each example is\n",
    "            an `n_c` by `n_in` matrix (e.g., the matrix of `n_c` context\n",
    "            embeddings, each of dimensionality `n_in`, for a CBOW model).\n",
    "        target : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex,)`\n",
    "            Integer indices of the target class(es) for each example in the\n",
    "            minibatch (e.g., the target word id for an example in a CBOW model).\n",
    "        neg_samples : :py:class:`ndarray <numpy.ndarray>` of shape (`num_negative_samples`,) or None\n",
    "            An optional array of negative samples to use during the loss\n",
    "            calculation. These will be used instead of samples draw from\n",
    "            ``self.noise_sampler``. Default is None.\n",
    "        retain_derived : bool\n",
    "            Whether to retain the variables calculated during the forward pass\n",
    "            for use later during backprop. If False, this suggests the layer\n",
    "            will not be expected to backprop through with regard to this input.\n",
    "            Default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            The NCE loss summed over the minibatch and samples.\n",
    "        y_pred : :py:class:`ndarray <numpy.ndarray>` of shape (`n_ex`, `n_c`)\n",
    "            The network predictions for the conditional probability of each\n",
    "            target given each context: entry (`i`, `j`) gives the predicted\n",
    "            probability of target `i` under context vector `j`.\n",
    "        \"\"\"\n",
    "        if not self.is_initialized:\n",
    "            self.n_in = X.shape[-1]\n",
    "            self._init_params()\n",
    "\n",
    "        loss, Z_target, Z_neg, y_pred, y_true, noise_samples = self._loss(\n",
    "            X, target, neg_samples\n",
    "        )\n",
    "\n",
    "        # cache derived variables for gradient calculation\n",
    "        if retain_derived:\n",
    "            self.X.append(X)\n",
    "\n",
    "            self.derived_variables[\"y_pred\"].append(y_pred)\n",
    "            self.derived_variables[\"target\"].append(target)\n",
    "            self.derived_variables[\"out_labels\"].append(y_true)\n",
    "            self.derived_variables[\"target_logits\"].append(Z_target)\n",
    "            self.derived_variables[\"noise_samples\"].append(noise_samples)\n",
    "            self.derived_variables[\"noise_logits\"].append(Z_neg)\n",
    "\n",
    "        return loss, np.squeeze(y_pred[..., :1], -1)\n",
    "\n",
    "    def _loss(self, X, target, neg_samples):\n",
    "        \"\"\"Actual computation of NCE loss\"\"\"\n",
    "        fstr = \"X must have shape (n_ex, n_c, n_in), but got {} dims instead\"\n",
    "        assert X.ndim == 3, fstr.format(X.ndim)\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "\n",
    "        # sample negative samples from the noise distribution\n",
    "        if neg_samples is None:\n",
    "            neg_samples = self.noise_sampler(self.num_negative_samples)\n",
    "        assert len(neg_samples) == self.num_negative_samples\n",
    "\n",
    "        # get the probability of the negative sample class and the target\n",
    "        # class under the noise distribution\n",
    "        p_neg_samples = self.noise_sampler.probs[neg_samples]\n",
    "        p_target = np.atleast_2d(self.noise_sampler.probs[target])\n",
    "\n",
    "        # save the noise samples for debugging\n",
    "        noise_samples = (neg_samples, p_target, p_neg_samples)\n",
    "\n",
    "        # compute the logit for the negative samples and target\n",
    "        Z_target = X @ W[target].T + b[0, target]\n",
    "        Z_neg = X @ W[neg_samples].T + b[0, neg_samples]\n",
    "\n",
    "        # subtract the log probability of each label under the noise dist\n",
    "        if self.subtract_log_label_prob:\n",
    "            n, m = Z_target.shape[0], Z_neg.shape[0]\n",
    "            Z_target[range(n), ...] -= np.log(p_target)\n",
    "            Z_neg[range(m), ...] -= np.log(p_neg_samples)\n",
    "\n",
    "        # only retain the probability of the target under its associated\n",
    "        # minibatch example\n",
    "        aa, _, cc = Z_target.shape\n",
    "        Z_target = Z_target[range(aa), :, range(cc)][..., None]\n",
    "\n",
    "        # p_target = (n_ex, n_c, 1)\n",
    "        # p_neg = (n_ex, n_c, n_samples)\n",
    "        pred_p_target = self.act_fn(Z_target)\n",
    "        pred_p_neg = self.act_fn(Z_neg)\n",
    "\n",
    "        # if we're in evaluation mode, ignore the negative samples - just\n",
    "        # return the binary cross entropy on the targets\n",
    "        y_pred = pred_p_target\n",
    "        # (n_ex, n_c, 1 + n_samples) (target is first column)\n",
    "        y_pred = np.concatenate((y_pred, pred_p_neg), axis=-1)\n",
    "\n",
    "        n_targets = 1\n",
    "        y_true = np.zeros_like(y_pred)\n",
    "        y_true[..., :n_targets] = 1\n",
    "\n",
    "        # binary cross entropy\n",
    "        eps = np.finfo(float).eps\n",
    "        np.clip(y_pred, eps, 1 - eps, y_pred)\n",
    "        loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss, Z_target, Z_neg, y_pred, y_true, noise_samples\n",
    "\n",
    "    def grad(self, retain_grads=True, update_params=True):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the NCE loss with regard to the inputs,\n",
    "        weights, and biases.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        retain_grads : bool\n",
    "            Whether to include the intermediate parameter gradients computed\n",
    "            during the backward pass in the final parameter update. Default is\n",
    "            True.\n",
    "        update_params : bool\n",
    "            Whether to perform a single step of gradient descent on the layer\n",
    "            weights and bias using the calculated gradients. If `retain_grads`\n",
    "            is False, this option is ignored and the parameter gradients are\n",
    "            not updated. Default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dLdX : :py:class:`ndarray <numpy.ndarray>` of shape (`n_ex`, `n_in`) or list of arrays\n",
    "            The gradient of the loss with regard to the layer input(s) `X`.\n",
    "        \"\"\"\n",
    "\n",
    "        dX = []\n",
    "        for input_idx, x in enumerate(self.X):\n",
    "            dx, dw, db = self._grad(x, input_idx)\n",
    "            dX.append(dx)\n",
    "\n",
    "            if retain_grads:\n",
    "                self.gradients[\"W\"] += dw\n",
    "                self.gradients[\"b\"] += db\n",
    "\n",
    "        dX = dX[0] if len(self.X) == 1 else dX\n",
    "\n",
    "        if retain_grads and update_params:\n",
    "            self.update()\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def _grad(self, X, input_idx):\n",
    "        \"\"\"Actual computation of gradient wrt. loss weights + input\"\"\"\n",
    "        W, b = self.parameters[\"W\"], self.parameters[\"b\"]\n",
    "\n",
    "        y_pred = self.derived_variables[\"y_pred\"][input_idx]\n",
    "        target = self.derived_variables[\"target\"][input_idx]\n",
    "        y_true = self.derived_variables[\"out_labels\"][input_idx]\n",
    "        Z_neg = self.derived_variables[\"noise_logits\"][input_idx]\n",
    "        Z_target = self.derived_variables[\"target_logits\"][input_idx]\n",
    "        neg_samples = self.derived_variables[\"noise_samples\"][input_idx][0]\n",
    "\n",
    "        # the number of target classes per minibatch example\n",
    "        n_targets = 1\n",
    "\n",
    "        # calculate the grad of the binary cross entropy wrt. the network\n",
    "        # predictions\n",
    "        preds, classes = y_pred.flatten(), y_true.flatten()\n",
    "\n",
    "        dLdp_real = ((1 - classes) / (1 - preds)) - (classes / preds)\n",
    "        dLdp_real = dLdp_real.reshape(*y_pred.shape)\n",
    "\n",
    "        # partition the gradients into target and negative sample portions\n",
    "        dLdy_pred_target = dLdp_real[..., :n_targets]\n",
    "        dLdy_pred_neg = dLdp_real[..., n_targets:]\n",
    "\n",
    "        # compute gradients of the loss wrt the data and noise logits\n",
    "        dLdZ_target = dLdy_pred_target * self.act_fn.grad(Z_target)\n",
    "        dLdZ_neg = dLdy_pred_neg * self.act_fn.grad(Z_neg)\n",
    "\n",
    "        # compute param gradients on target + negative samples\n",
    "        dB_neg = dLdZ_neg.sum(axis=(0, 1))\n",
    "        dB_target = dLdZ_target.sum(axis=(1, 2))\n",
    "\n",
    "        dW_neg = (dLdZ_neg.transpose(0, 2, 1) @ X).sum(axis=0)\n",
    "        dW_target = (dLdZ_target.transpose(0, 2, 1) @ X).sum(axis=1)\n",
    "\n",
    "        # TODO: can this be done with np.einsum instead?\n",
    "        dX_target = np.vstack(\n",
    "            [dLdZ_target[[ix]] @ W[[t]] for ix, t in enumerate(target)]\n",
    "        )\n",
    "        dX_neg = dLdZ_neg @ W[neg_samples]\n",
    "\n",
    "        hits = list(set(target).intersection(set(neg_samples)))\n",
    "        hit_ixs = [np.where(target == h)[0] for h in hits]\n",
    "\n",
    "        # adjust param gradients if there's an accidental hit\n",
    "        if len(hits) != 0:\n",
    "            hit_ixs = np.concatenate(hit_ixs)\n",
    "            target = np.delete(target, hit_ixs)\n",
    "            dB_target = np.delete(dB_target, hit_ixs)\n",
    "            dW_target = np.delete(dW_target, hit_ixs, 0)\n",
    "\n",
    "        dX = dX_target + dX_neg\n",
    "\n",
    "        # use np.add.at to ensure that repeated indices in the target (or\n",
    "        # possibly in neg_samples if sampling is done with replacement) are\n",
    "        # properly accounted for\n",
    "        dB = np.zeros_like(b).flatten()\n",
    "        np.add.at(dB, target, dB_target)\n",
    "        np.add.at(dB, neg_samples, dB_neg)\n",
    "        dB = dB.reshape(*b.shape)\n",
    "\n",
    "        dW = np.zeros_like(W)\n",
    "        np.add.at(dW, target, dW_target)\n",
    "        np.add.at(dW, neg_samples, dW_neg)\n",
    "\n",
    "        return dX, dW, dB\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyter)",
   "language": "python",
   "name": "pycharm-34f9d306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
