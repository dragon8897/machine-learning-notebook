{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Cell\n",
    "\n",
    "- 正向传播\n",
    "$$\n",
    "\\begin{align*}\n",
    "a^{<t>} &= tanh(W_{ax}x^{<t>} + W_{aa}a^{<t-1>} + b_a) \\\\\n",
    "\\hat{y}^{<t>} &= softmax(W_{ya}a^{<t>} + b_y) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- 反向传播\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial a^{<t>}}{\\partial W_{ax}} &= (1 - \\tanh(W_{ax}x^{<t>} + W_{aa}a^{<t-1>} + b)^2)\\cdot x^{<t>T} \\\\\n",
    "\\frac{\\partial a^{<t>}}{\\partial W_{aa}} &= (1 - \\tanh(W_{ax}x^{<t>} + W_{aa}a^{<t-1>} + b)^2)\\cdot a^{<t-1>T} \\\\\n",
    "\\frac{\\partial a^{<t>}}{\\partial b} &= \\sum_{batch}(1 - \\tanh(W_{ax}x^{<t>} + W_{aa}a^{<t-1>} + b)^2) \\\\\n",
    "\\frac{\\partial a^{<t>}}{\\partial x^{<t>}} &= W_{ax}^T \\cdot (1 - \\tanh(W_{ax}x^{<t>} + W_{aa}a^{<t-1>} + b)^2) \\\\\n",
    "\\frac{\\partial a^{<t>}}{\\partial x^{<t-1>}} &= W_{aa}^T \\cdot (1 - \\tanh(W_{ax}x^{<t>} + W_{aa}a^{<t-1>} + b)^2) \\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell():\n",
    "    def __init__(self, n_out, act_fn=\"Tanh\", init=\"glorot_uniform\", optimizer=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_out : int\n",
    "            The dimension of a single hidden state / output on a given timestep\n",
    "        act_fn : str, :doc:`Activation <numpy_ml.neural_nets.activations>` object, or None\n",
    "            The activation function for computing ``A[t]``. Default is `'Tanh'`.\n",
    "        init : {'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'}\n",
    "            The weight initialization strategy. Default is `'glorot_uniform'`.\n",
    "        optimizer : str, :doc:`Optimizer <numpy_ml.neural_nets.optimizers>` object, or None\n",
    "            The optimization strategy to use when performing gradient updates\n",
    "            within the :meth:`update` method.  If None, use the :class:`SGD\n",
    "            <numpy_ml.neural_nets.optimizers.SGD>` optimizer with default\n",
    "            parameters. Default is None.\n",
    "        \"\"\"\n",
    "        self.init = init\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.n_timesteps = None\n",
    "        self.act_fn = ActivationInitializer(act_fn)()\n",
    "        self.parameters = {\"Waa\": None, \"Wax\": None, \"ba\": None, \"bx\": None}\n",
    "        self.is_initialized = False\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.X = []\n",
    "        init_weights = WeightInitializer(str(self.act_fn), mode=self.init)\n",
    "\n",
    "        Wax = init_weights((self.n_in, self.n_out))\n",
    "        Waa = init_weights((self.n_out, self.n_out))\n",
    "        ba = np.zeros((self.n_out, 1))\n",
    "        bx = np.zeros((self.n_out, 1))\n",
    "\n",
    "        self.parameters = {\"Waa\": Waa, \"Wax\": Wax, \"ba\": ba, \"bx\": bx}\n",
    "\n",
    "        self.gradients = {\n",
    "            \"Waa\": np.zeros_like(Waa),\n",
    "            \"Wax\": np.zeros_like(Wax),\n",
    "            \"ba\": np.zeros_like(ba),\n",
    "            \"bx\": np.zeros_like(bx),\n",
    "        }\n",
    "\n",
    "        self.derived_variables = {\n",
    "            \"A\": [],\n",
    "            \"Z\": [],\n",
    "            \"n_timesteps\": 0,\n",
    "            \"current_step\": 0,\n",
    "            \"dLdA_accumulator\": None,\n",
    "        }\n",
    "\n",
    "        self.is_initialized = True\n",
    "\n",
    "    def forward(self, Xt):\n",
    "        \"\"\"\n",
    "        Compute the network output for a single timestep.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Xt : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, n_in)`\n",
    "            Input at timestep `t` consisting of `n_ex` examples each of\n",
    "            dimensionality `n_in`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        At: :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, n_out)`\n",
    "            The value of the hidden state at timestep `t` for each of the\n",
    "            `n_ex` examples.\n",
    "        \"\"\"\n",
    "        if not self.is_initialized:\n",
    "            self.n_in = Xt.shape[1]\n",
    "            self._init_params()\n",
    "\n",
    "        # increment timestep\n",
    "        self.derived_variables[\"n_timesteps\"] += 1\n",
    "        self.derived_variables[\"current_step\"] += 1\n",
    "\n",
    "        # Retrieve parameters\n",
    "        ba = self.parameters[\"ba\"]\n",
    "        bx = self.parameters[\"bx\"]\n",
    "        Wax = self.parameters[\"Wax\"]\n",
    "        Waa = self.parameters[\"Waa\"]\n",
    "\n",
    "        # initialize the hidden state to zero\n",
    "        As = self.derived_variables[\"A\"]\n",
    "        if len(As) == 0:\n",
    "            n_ex, n_in = Xt.shape\n",
    "            A0 = np.zeros((n_ex, self.n_out))\n",
    "            As.append(A0)\n",
    "\n",
    "        # compute next hidden state\n",
    "        Zt = As[-1] @ Waa + ba.T + Xt @ Wax + bx.T\n",
    "        At = self.act_fn(Zt)\n",
    "\n",
    "        self.derived_variables[\"Z\"].append(Zt)\n",
    "        self.derived_variables[\"A\"].append(At)\n",
    "\n",
    "        # store intermediate variables\n",
    "        self.X.append(Xt)\n",
    "        return At\n",
    "\n",
    "    def backward(self, dLdAt):\n",
    "        \"\"\"\n",
    "        Backprop for a single timestep.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdAt : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, n_out)`\n",
    "            The gradient of the loss wrt. the layer outputs (ie., hidden\n",
    "            states) at timestep `t`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dLdXt : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, n_in)`\n",
    "            The gradient of the loss wrt. the layer inputs at timestep `t`.\n",
    "        \"\"\"\n",
    "        assert self.trainable, \"Layer is frozen\"\n",
    "\n",
    "        #  decrement current step\n",
    "        self.derived_variables[\"current_step\"] -= 1\n",
    "\n",
    "        # extract context variables\n",
    "        Zs = self.derived_variables[\"Z\"]\n",
    "        As = self.derived_variables[\"A\"]\n",
    "        t = self.derived_variables[\"current_step\"]\n",
    "        dA_acc = self.derived_variables[\"dLdA_accumulator\"]\n",
    "\n",
    "        # initialize accumulator\n",
    "        if dA_acc is None:\n",
    "            dA_acc = np.zeros_like(As[0])\n",
    "\n",
    "        # get network weights for gradient calcs\n",
    "        Wax = self.parameters[\"Wax\"]\n",
    "        Waa = self.parameters[\"Waa\"]\n",
    "\n",
    "        # compute gradient components at timestep t\n",
    "        dA = dLdAt + dA_acc\n",
    "        dZ = self.act_fn.grad(Zs[t]) * dA\n",
    "        dXt = dZ @ Wax.T\n",
    "\n",
    "        # update parameter gradients with signal from current step\n",
    "        self.gradients[\"Waa\"] += As[t].T @ dZ\n",
    "        self.gradients[\"Wax\"] += self.X[t].T @ dZ\n",
    "        self.gradients[\"ba\"] += dZ.sum(axis=0, keepdims=True).T\n",
    "        self.gradients[\"bx\"] += dZ.sum(axis=0, keepdims=True).T\n",
    "\n",
    "        # update accumulator variable for hidden state\n",
    "        self.derived_variables[\"dLdA_accumulator\"] = dZ @ Waa.T\n",
    "        return dXt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU\n",
    "\n",
    "- 正向传播\n",
    "$$\n",
    "  \\begin{align*}\n",
    "  z_t &= \\sigma(W_t\\cdot[h_{t-1}, x_t]) \\\\\n",
    "  r_t &= \\sigma(W_t\\cdot[h_{t-1}, x_t]) \\\\\n",
    "  \\tilde h_t &= \\tanh(W\\cdot[r_t * h_{t-1}, x_t]) \\\\\n",
    "  h_t &= (1 - z_t) * h_{t-1} + z_t * \\tilde h_t \\\\\n",
    "  \\end{align*}\n",
    "$$\n",
    "- 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTSM\n",
    "\n",
    "- 正向传播\n",
    "    $$\n",
    "    \\Gamma_f^{\\langle t \\rangle} = \\sigma(W_f[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f) \\\\\n",
    "    \\Gamma_u^{\\langle t \\rangle} = \\sigma(W_u[a^{\\langle t-1 \\rangle}, x^{\\{t\\}}] + b_u) \\\\\n",
    "    \\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c) \\\\\n",
    "    c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle}* c^{\\langle t-1 \\rangle} + \\Gamma_u^{\\langle t \\rangle} *\\tilde{c}^{\\langle t \\rangle} \\\\\n",
    "    \\Gamma_o^{\\langle t \\rangle}=  \\sigma(W_o[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o) \\\\\n",
    "    a^{\\langle t \\rangle} = \\Gamma_o^{\\langle t \\rangle}* \\tanh(c^{\\langle t \\rangle}) \\\\\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(LayerBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_out,\n",
    "        act_fn=\"Tanh\",\n",
    "        gate_fn=\"Sigmoid\",\n",
    "        init=\"glorot_uniform\",\n",
    "        optimizer=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A single step of a long short-term memory (LSTM) RNN.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Notation:\n",
    "\n",
    "        - ``Z[t]``  is the input to each of the gates at timestep `t`\n",
    "        - ``A[t]``  is the value of the hidden state at timestep `t`\n",
    "        - ``Cc[t]`` is the value of the *candidate* cell/memory state at timestep `t`\n",
    "        - ``C[t]``  is the value of the *final* cell/memory state at timestep `t`\n",
    "        - ``Gf[t]`` is the output of the forget gate at timestep `t`\n",
    "        - ``Gu[t]`` is the output of the update gate at timestep `t`\n",
    "        - ``Go[t]`` is the output of the output gate at timestep `t`\n",
    "\n",
    "        Equations::\n",
    "\n",
    "            Z[t]  = stack([A[t-1], X[t]])\n",
    "            Gf[t] = gate_fn(Wf @ Z[t] + bf)\n",
    "            Gu[t] = gate_fn(Wu @ Z[t] + bu)\n",
    "            Go[t] = gate_fn(Wo @ Z[t] + bo)\n",
    "            Cc[t] = act_fn(Wc @ Z[t] + bc)\n",
    "            C[t]  = Gf[t] * C[t-1] + Gu[t] * Cc[t]\n",
    "            A[t]  = Go[t] * act_fn(C[t])\n",
    "\n",
    "        where `@` indicates dot/matrix product, and '*' indicates elementwise\n",
    "        multiplication.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_out : int\n",
    "            The dimension of a single hidden state / output on a given timestep.\n",
    "        act_fn : str, :doc:`Activation <numpy_ml.neural_nets.activations>` object, or None\n",
    "            The activation function for computing ``A[t]``. Default is\n",
    "            `'Tanh'`.\n",
    "        gate_fn : str, :doc:`Activation <numpy_ml.neural_nets.activations>` object, or None\n",
    "            The gate function for computing the update, forget, and output\n",
    "            gates. Default is `'Sigmoid'`.\n",
    "        init : {'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'}\n",
    "            The weight initialization strategy. Default is `'glorot_uniform'`.\n",
    "        optimizer : str, :doc:`Optimizer <numpy_ml.neural_nets.optimizers>` object, or None\n",
    "            The optimization strategy to use when performing gradient updates\n",
    "            within the :meth:`update` method.  If None, use the :class:`SGD\n",
    "            <numpy_ml.neural_nets.optimizers.SGD>` optimizer with default\n",
    "            parameters. Default is None.\n",
    "        \"\"\"\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "        self.init = init\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.n_timesteps = None\n",
    "        self.act_fn = ActivationInitializer(act_fn)()\n",
    "        self.gate_fn = ActivationInitializer(gate_fn)()\n",
    "        self.parameters = {\n",
    "            \"Wf\": None,\n",
    "            \"Wu\": None,\n",
    "            \"Wc\": None,\n",
    "            \"Wo\": None,\n",
    "            \"bf\": None,\n",
    "            \"bu\": None,\n",
    "            \"bc\": None,\n",
    "            \"bo\": None,\n",
    "        }\n",
    "        self.is_initialized = False\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.X = []\n",
    "        init_weights_gate = WeightInitializer(str(self.gate_fn), mode=self.init)\n",
    "        init_weights_act = WeightInitializer(str(self.act_fn), mode=self.init)\n",
    "\n",
    "        Wf = init_weights_gate((self.n_in + self.n_out, self.n_out))\n",
    "        Wu = init_weights_gate((self.n_in + self.n_out, self.n_out))\n",
    "        Wc = init_weights_act((self.n_in + self.n_out, self.n_out))\n",
    "        Wo = init_weights_gate((self.n_in + self.n_out, self.n_out))\n",
    "\n",
    "        bf = np.zeros((1, self.n_out))\n",
    "        bu = np.zeros((1, self.n_out))\n",
    "        bc = np.zeros((1, self.n_out))\n",
    "        bo = np.zeros((1, self.n_out))\n",
    "\n",
    "        self.parameters = {\n",
    "            \"Wf\": Wf,\n",
    "            \"Wu\": Wu,\n",
    "            \"Wc\": Wc,\n",
    "            \"Wo\": Wo,\n",
    "            \"bf\": bf,\n",
    "            \"bu\": bu,\n",
    "            \"bc\": bc,\n",
    "            \"bo\": bo,\n",
    "        }\n",
    "\n",
    "        self.gradients = {\n",
    "            \"Wf\": np.zeros_like(Wf),\n",
    "            \"Wu\": np.zeros_like(Wu),\n",
    "            \"Wc\": np.zeros_like(Wc),\n",
    "            \"Wo\": np.zeros_like(Wo),\n",
    "            \"bf\": np.zeros_like(bf),\n",
    "            \"bu\": np.zeros_like(bu),\n",
    "            \"bc\": np.zeros_like(bc),\n",
    "            \"bo\": np.zeros_like(bo),\n",
    "        }\n",
    "\n",
    "        self.derived_variables = {\n",
    "            \"C\": [],\n",
    "            \"A\": [],\n",
    "            \"Gf\": [],\n",
    "            \"Gu\": [],\n",
    "            \"Go\": [],\n",
    "            \"Gc\": [],\n",
    "            \"Cc\": [],\n",
    "            \"n_timesteps\": 0,\n",
    "            \"current_step\": 0,\n",
    "            \"dLdA_accumulator\": None,\n",
    "            \"dLdC_accumulator\": None,\n",
    "        }\n",
    "\n",
    "        self.is_initialized = True\n",
    "\n",
    "    def _get_params(self):\n",
    "        Wf = self.parameters[\"Wf\"]\n",
    "        Wu = self.parameters[\"Wu\"]\n",
    "        Wc = self.parameters[\"Wc\"]\n",
    "        Wo = self.parameters[\"Wo\"]\n",
    "        bf = self.parameters[\"bf\"]\n",
    "        bu = self.parameters[\"bu\"]\n",
    "        bc = self.parameters[\"bc\"]\n",
    "        bo = self.parameters[\"bo\"]\n",
    "        return Wf, Wu, Wc, Wo, bf, bu, bc, bo\n",
    "\n",
    "    @property\n",
    "    def hyperparameters(self):\n",
    "        \"\"\"Return a dictionary containing the layer hyperparameters.\"\"\"\n",
    "        return {\n",
    "            \"layer\": \"LSTMCell\",\n",
    "            \"init\": self.init,\n",
    "            \"n_in\": self.n_in,\n",
    "            \"n_out\": self.n_out,\n",
    "            \"act_fn\": str(self.act_fn),\n",
    "            \"gate_fn\": str(self.gate_fn),\n",
    "            \"optimizer\": {\n",
    "                \"cache\": self.optimizer.cache,\n",
    "                \"hyperparameters\": self.optimizer.hyperparameters,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def forward(self, Xt):\n",
    "        \"\"\"\n",
    "        Compute the layer output for a single timestep.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Xt : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, n_in)`\n",
    "            Input at timestep t consisting of `n_ex` examples each of\n",
    "            dimensionality `n_in`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        At: :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, n_out)`\n",
    "            The value of the hidden state at timestep `t` for each of the `n_ex`\n",
    "            examples.\n",
    "        Ct: :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, n_out)`\n",
    "            The value of the cell/memory state at timestep `t` for each of the\n",
    "            `n_ex` examples.\n",
    "        \"\"\"\n",
    "        if not self.is_initialized:\n",
    "            self.n_in = Xt.shape[1]\n",
    "            self._init_params()\n",
    "\n",
    "        Wf, Wu, Wc, Wo, bf, bu, bc, bo = self._get_params()\n",
    "\n",
    "        self.derived_variables[\"n_timesteps\"] += 1\n",
    "        self.derived_variables[\"current_step\"] += 1\n",
    "\n",
    "        if len(self.derived_variables[\"A\"]) == 0:\n",
    "            n_ex, n_in = Xt.shape\n",
    "            init = np.zeros((n_ex, self.n_out))\n",
    "            self.derived_variables[\"A\"].append(init)\n",
    "            self.derived_variables[\"C\"].append(init)\n",
    "\n",
    "        A_prev = self.derived_variables[\"A\"][-1]\n",
    "        C_prev = self.derived_variables[\"C\"][-1]\n",
    "\n",
    "        # concatenate A_prev and Xt to create Zt\n",
    "        Zt = np.hstack([A_prev, Xt])\n",
    "\n",
    "        Gft = self.gate_fn(Zt @ Wf + bf)\n",
    "        Gut = self.gate_fn(Zt @ Wu + bu)\n",
    "        Got = self.gate_fn(Zt @ Wo + bo)\n",
    "        Cct = self.act_fn(Zt @ Wc + bc)\n",
    "        Ct = Gft * C_prev + Gut * Cct\n",
    "        At = Got * self.act_fn(Ct)\n",
    "\n",
    "        # bookkeeping\n",
    "        self.X.append(Xt)\n",
    "        self.derived_variables[\"A\"].append(At)\n",
    "        self.derived_variables[\"C\"].append(Ct)\n",
    "        self.derived_variables[\"Gf\"].append(Gft)\n",
    "        self.derived_variables[\"Gu\"].append(Gut)\n",
    "        self.derived_variables[\"Go\"].append(Got)\n",
    "        self.derived_variables[\"Cc\"].append(Cct)\n",
    "        return At, Ct\n",
    "\n",
    "    def backward(self, dLdAt):\n",
    "        \"\"\"\n",
    "        Backprop for a single timestep.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdAt : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, n_out)`\n",
    "            The gradient of the loss wrt. the layer outputs (ie., hidden\n",
    "            states) at timestep `t`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dLdXt : :py:class:`ndarray <numpy.ndarray>` of shape `(n_ex, n_in)`\n",
    "            The gradient of the loss wrt. the layer inputs at timestep `t`.\n",
    "        \"\"\"\n",
    "        assert self.trainable, \"Layer is frozen\"\n",
    "\n",
    "        Wf, Wu, Wc, Wo, bf, bu, bc, bo = self._get_params()\n",
    "\n",
    "        self.derived_variables[\"current_step\"] -= 1\n",
    "        t = self.derived_variables[\"current_step\"]\n",
    "\n",
    "        Got = self.derived_variables[\"Go\"][t]\n",
    "        Gft = self.derived_variables[\"Gf\"][t]\n",
    "        Gut = self.derived_variables[\"Gu\"][t]\n",
    "        Cct = self.derived_variables[\"Cc\"][t]\n",
    "        At = self.derived_variables[\"A\"][t + 1]\n",
    "        Ct = self.derived_variables[\"C\"][t + 1]\n",
    "        C_prev = self.derived_variables[\"C\"][t]\n",
    "        A_prev = self.derived_variables[\"A\"][t]\n",
    "\n",
    "        Xt = self.X[t]\n",
    "        Zt = np.hstack([A_prev, Xt])\n",
    "\n",
    "        dA_acc = self.derived_variables[\"dLdA_accumulator\"]\n",
    "        dC_acc = self.derived_variables[\"dLdC_accumulator\"]\n",
    "\n",
    "        # initialize accumulators\n",
    "        if dA_acc is None:\n",
    "            dA_acc = np.zeros_like(At)\n",
    "\n",
    "        if dC_acc is None:\n",
    "            dC_acc = np.zeros_like(Ct)\n",
    "\n",
    "        # Gradient calculations\n",
    "        # ---------------------\n",
    "\n",
    "        dA = dLdAt + dA_acc\n",
    "        dC = dC_acc + dA * Got * self.act_fn.grad(Ct)\n",
    "\n",
    "        # compute the input to the gate functions at timestep t\n",
    "        _Go = Zt @ Wo + bo\n",
    "        _Gf = Zt @ Wf + bo\n",
    "        _Gu = Zt @ Wu + bo\n",
    "        _Gc = Zt @ Wc + bc\n",
    "\n",
    "        # compute gradients wrt the *input* to each gate\n",
    "        dGot = dA * self.act_fn(Ct) * self.gate_fn.grad(_Go)\n",
    "        dCct = dC * Gut * self.act_fn.grad(_Gc)\n",
    "        dGut = dC * Cct * self.gate_fn.grad(_Gu)\n",
    "        dGft = dC * C_prev * self.gate_fn.grad(_Gf)\n",
    "\n",
    "        dZ = dGft @ Wf.T + dGut @ Wu.T + dCct @ Wc.T + dGot @ Wo.T\n",
    "        dXt = dZ[:, self.n_out :]\n",
    "\n",
    "        self.gradients[\"Wc\"] += Zt.T @ dCct\n",
    "        self.gradients[\"Wu\"] += Zt.T @ dGut\n",
    "        self.gradients[\"Wf\"] += Zt.T @ dGft\n",
    "        self.gradients[\"Wo\"] += Zt.T @ dGot\n",
    "        self.gradients[\"bo\"] += dGot.sum(axis=0, keepdims=True)\n",
    "        self.gradients[\"bu\"] += dGut.sum(axis=0, keepdims=True)\n",
    "        self.gradients[\"bf\"] += dGft.sum(axis=0, keepdims=True)\n",
    "        self.gradients[\"bc\"] += dCct.sum(axis=0, keepdims=True)\n",
    "\n",
    "        self.derived_variables[\"dLdA_accumulator\"] = dZ[:, : self.n_out]\n",
    "        self.derived_variables[\"dLdC_accumulator\"] = Gft * dC\n",
    "        return dXt\n",
    "\n",
    "    def flush_gradients(self):\n",
    "        assert self.trainable, \"Layer is frozen\"\n",
    "\n",
    "        self.X = []\n",
    "        for k, v in self.derived_variables.items():\n",
    "            self.derived_variables[k] = []\n",
    "\n",
    "        self.derived_variables[\"n_timesteps\"] = 0\n",
    "        self.derived_variables[\"current_step\"] = 0\n",
    "\n",
    "        # reset parameter gradients to 0\n",
    "        for k, v in self.parameters.items():\n",
    "            self.gradients[k] = np.zeros_like(v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyter)",
   "language": "python",
   "name": "pycharm-34f9d306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
