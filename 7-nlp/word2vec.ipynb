{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*- coding:utf-8 -*-\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Word2Vec:\n",
    "    \n",
    "    def __init__(self, train_data=None, word_size=128, window=5, min_count=5, model='cbow', shared_softmax=True, nb_negative=16, epochs=2, batch_size=8000):\n",
    "        if train_data: #如果有数据传入，那么启动建模和训练程序；如果没有，那么只初始化一个空对象。\n",
    "            self.train_data = train_data\n",
    "            self.word_size = word_size\n",
    "            self.window = window\n",
    "            self.min_count = min_count\n",
    "            self.model = model\n",
    "            self.shared_softmax = shared_softmax\n",
    "            self.nb_negative = nb_negative\n",
    "            self.epochs = epochs\n",
    "            self.batch_size = batch_size\n",
    "            self.words = defaultdict(int)\n",
    "            self.word_count()\n",
    "            self.build_model()\n",
    "            self.train_model()\n",
    "            \n",
    "            \n",
    "    def word_count(self): #统计词频，过滤低频词\n",
    "        for total,t in enumerate(self.train_data):\n",
    "            for w in t:\n",
    "                self.words[w] += 1\n",
    "            if total % 10000 == 0:\n",
    "                print('%s, get %s articles, %s uique words.'%(datetime.datetime.now(), total, len(self.words)))\n",
    "        self.words = {i:j for i,j in self.words.items() if j >= self.min_count}\n",
    "        self.id2word = {i+1:j for i,j in enumerate(self.words.keys())}\n",
    "        self.word2id = {j:i for i,j in self.id2word.items()}\n",
    "        self.total_sentences = total + 1\n",
    "        self.total_words = len(self.word2id)\n",
    "        self.total_word_frequency = sum(self.words.values())\n",
    "        print('%s, min_count=%s left %s unique words.'%(datetime.datetime.now(), self.min_count, self.total_words))\n",
    "        \n",
    "        \n",
    "    def random_softmax_loss(self, nb_negative, inputs, targets, weights, biases=None): #定义loss函数：随机抽样、点乘、交叉熵。\n",
    "        nb_classes, real_batch_size = tf.shape(weights)[0], tf.shape(targets)[0]\n",
    "        negative_sample = tf.random_uniform([real_batch_size, nb_negative], 0, nb_classes, dtype=tf.int32)\n",
    "        random_sample = tf.concat([targets, negative_sample], axis=1)\n",
    "        sampled_weights = tf.nn.embedding_lookup(weights, random_sample)\n",
    "        if biases:\n",
    "            sampled_biases = tf.nn.embedding_lookup(biases, random_sample)\n",
    "            sampled_logits = tf.matmul(inputs, sampled_weights, transpose_b=True)[:,0,:] + sampled_biases\n",
    "        else:\n",
    "            sampled_logits = tf.matmul(inputs, sampled_weights, transpose_b=True)[:,0,:]\n",
    "        sampled_labels = tf.zeros([real_batch_size], dtype=tf.int32)\n",
    "        return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=sampled_logits, labels=sampled_labels))\n",
    "    \n",
    "    \n",
    "    def build_model(self): #建立模型，比较常规\n",
    "        self.sess = tf.Session()\n",
    "        self.embeddings = tf.Variable(tf.random_uniform([self.total_words+1, self.word_size], -0.05, 0.05))\n",
    "        self.normalized_embeddings = tf.nn.l2_normalize(self.embeddings, 1)\n",
    "        self.target_words = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "        if self.model == 'cbow':\n",
    "            self.input_words = tf.placeholder(tf.int32, shape=[None, 2*self.window])\n",
    "            self.input_vecs = tf.nn.embedding_lookup(self.embeddings, self.input_words)\n",
    "            self.input_vecs = tf.expand_dims(tf.reduce_sum(self.input_vecs, 1), 1)\n",
    "        else:\n",
    "            self.input_words = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "            self.input_vecs = tf.nn.embedding_lookup(self.embeddings, self.input_words)\n",
    "        if self.shared_softmax:\n",
    "            self.loss = self.random_softmax_loss(self.nb_negative, self.input_vecs, self.target_words, self.embeddings)\n",
    "        else:\n",
    "            self.softmax_weights = tf.Variable(tf.random_uniform([self.total_words+1, self.word_size], -0.05, 0.05))\n",
    "            self.softmax_biases = tf.Variable(tf.zeros([self.total_words+1]))\n",
    "            self.loss = self.random_softmax_loss(self.nb_negative, self.input_vecs, self.target_words, self.softmax_weights, self.softmax_biases)\n",
    "            \n",
    "            \n",
    "    def data_generator(self): #数据生成器，用来生成每个batch\n",
    "        if self.model == 'cbow':\n",
    "            x,y = [],[]\n",
    "            for idx,t in enumerate(self.train_data):\n",
    "                t = [self.word2id[i] for i in t if i in self.word2id]\n",
    "                for i,s in enumerate(t):\n",
    "                    win = t[max(0, i-self.window): i] + t[i+1: min(len(t), i+self.window)]\n",
    "                    win += [0]*(2*self.window-len(win))\n",
    "                    x.append(win)\n",
    "                    y.append([s])\n",
    "                    if len(x) == self.batch_size:\n",
    "                        yield idx, np.array(x), np.array(y)\n",
    "                        x,y = [],[]\n",
    "            if x:\n",
    "                yield idx, np.array(x), np.array(y)\n",
    "        else:\n",
    "            x,y = [],[]\n",
    "            for idx,t in enumerate(self.train_data):\n",
    "                t = [self.word2id[i] for i in t if i in self.word2id]\n",
    "                for i,s in enumerate(t):\n",
    "                    win = t[max(0, i-self.window): i] + t[i+1: min(len(t), i+self.window)]\n",
    "                    for w in win:\n",
    "                        x.append([w])\n",
    "                        y.append([s])\n",
    "                        if len(x) == self.batch_size:\n",
    "                            yield idx, np.array(x), np.array(y)\n",
    "                            x,y = [],[]\n",
    "            if x:\n",
    "                yield idx, np.array(x), np.array(y)\n",
    "                \n",
    "                \n",
    "    def train_model(self): #模型训练部分，主要使用了多进程，一个进程负责生成数据，一个负责训练数据\n",
    "        from multiprocessing import Process,Queue\n",
    "        self.train_step = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        for e in range(self.epochs):\n",
    "            queue = Queue(1000)\n",
    "            def put_into_queue(queue):\n",
    "                data = self.data_generator()\n",
    "                for d in data:\n",
    "                    while queue.full():\n",
    "                        pass\n",
    "                    queue.put(d)\n",
    "            putting = Process(target=put_into_queue, args=(queue,))\n",
    "            putting.start()\n",
    "            count = 0\n",
    "            while True:\n",
    "                idx,x,y = queue.get()\n",
    "                if count%100 == 0:\n",
    "                    loss_ = self.sess.run(self.loss, feed_dict={self.input_words: x, self.target_words: y})\n",
    "                    print('%s, epoch %s, trained on %s articles, loss: %s'%(datetime.datetime.now(), e+1, idx, loss_))\n",
    "                self.sess.run(self.train_step, feed_dict={self.input_words: x, self.target_words: y})\n",
    "                count += 1\n",
    "                if idx+1 == self.total_sentences:\n",
    "                    break\n",
    "            putting.terminate()\n",
    "        self.embeddings = self.sess.run(self.embeddings) #训练完成后，重新初始化变量，使得在没有tf的环境中也能够调用模型\n",
    "        self.normalized_embeddings = self.sess.run(self.normalized_embeddings)\n",
    "        if not self.shared_softmax:\n",
    "            self.softmax_weights = self.sess.run(self.softmax_weights)\n",
    "            self.softmax_biases = self.sess.run(self.softmax_biases)\n",
    "            \n",
    "            \n",
    "    def __getitem__(self, w):\n",
    "        return self.embeddings[self.word2id[w]]\n",
    "    \n",
    "    \n",
    "    def most_similar(self, word, topn=10): #通过cos相似度找近义词\n",
    "        word_vec = self.normalized_embeddings[self.word2id[word]]\n",
    "        word_sim = np.dot(self.normalized_embeddings[1:], word_vec)\n",
    "        word_sim_argsort = word_sim.argsort()[::-1]\n",
    "        return [(self.id2word[i+1], word_sim[i]) for i in word_sim_argsort[1:topn+1]]\n",
    "    \n",
    "    \n",
    "    def log_proba(self, input_words): #由输入词预测词的概率分布对数\n",
    "        input_words = [self.embeddings[self.word2id[w]] for w in input_words if w in self.words]\n",
    "        if input_words:\n",
    "            if self.model == 'cbow': #对于cbow模型，直接将输入词向量求和然后点乘并softmax，这是很自然的。\n",
    "                input_words = sum(input_words)\n",
    "                if self.shared_softmax:\n",
    "                    logits = np.dot(self.embeddings, input_words)\n",
    "                else:\n",
    "                    logits = np.dot(self.softmax_weights, input_words) + self.softmax_biases\n",
    "                logits = np.exp(logits - logits.max())+1e-12\n",
    "                log_proba = np.log(logits/logits.sum())\n",
    "            else: #对于skip-gram模型，则利用贝叶斯公式和特征独立假设来算\n",
    "                logs = []\n",
    "                for v in input_words:\n",
    "                    if self.shared_softmax:\n",
    "                        logits = np.dot(self.embeddings, v)\n",
    "                    else:\n",
    "                        logits = np.dot(self.softmax_weights, v) + self.softmax_biases\n",
    "                    logits = np.exp(logits - logits.max())+1e-12\n",
    "                    logs.append(np.log(logits/logits.sum()))\n",
    "                log_proba = sum(logs) - np.array([0]+[(len(logs)-1)*(np.log(self.words[self.id2word[i]])-np.log(self.total_word_frequency)) for i in range(1,len(self.words)+1)])\n",
    "            log_proba_argsort = log_proba.argsort()[::-1]\n",
    "            return [(self.id2word[i], log_proba[i]) for i in log_proba_argsort if i != 0]\n",
    "        \n",
    "        \n",
    "    def save_model(self, saved_folder): #保存模型，模型不可再训练（事实上再训练对于多数人来说没有意义）\n",
    "        saved_folder = (saved_folder+'/').replace('//', '/')\n",
    "        pickle.dump([self.words, self.word2id, self.id2word, self.model, self.shared_softmax], open(saved_folder+'words.pickle', 'w'))\n",
    "        np.save(saved_folder+'embeddings', self.embeddings)\n",
    "        if not self.shared_softmax:\n",
    "            np.save(saved_folder+'softmax_weights', self.softmax_weights)\n",
    "            np.save(saved_folder+'softmax_biases', self.softmax_biases)\n",
    "            \n",
    "            \n",
    "    def load_model(self, saved_folder): #重新加载模型，但不可再训练\n",
    "        saved_folder = (saved_folder+'/').replace('//', '/')\n",
    "        self.words, self.word2id, self.id2word, self.model, self.shared_softmax = pickle.load(open(saved_folder+'words.pickle'))\n",
    "        self.embeddings = np.load(saved_folder+'embeddings.npy')\n",
    "        self.normalized_embeddings = self.embeddings/(self.embeddings**2).sum(axis=1).reshape((-1,1))**0.5\n",
    "        if not self.shared_softmax:\n",
    "            self.softmax_weights = np.load(saved_folder+'softmax_weights.npy')\n",
    "            self.softmax_biases = np.save(saved_folder+'softmax_biases.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新的 loss\n",
    "\n",
    "简介一下这个loss的来源和形式。这要从softmax为什么训练难度大开始说起～\n",
    "\n",
    "假设标签数（本文中也就是词表中词汇量）为n，那么\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(p_1,p_2,…,p_n) &= softmax(z_1,z_2,…,z_n) \\\\\n",
    "&= \\left(\\frac{e^{z_1}}{Z},\\frac{e^{z_2}}{Z},...,\\frac{e^{z_n}}{Z},\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "这里 $Z=e^z_1+e^z_2+⋯+e^z_n$ 。如果正确类别标签为t，使用交叉熵为loss，则\n",
    "$$\n",
    "L=−\\log\\frac{e^z_t}{Z}\n",
    "$$\n",
    "\n",
    "梯度为\n",
    "$$\n",
    "\\nabla L=−\\nabla z_t+ \\nabla(\\log Z)=−\\nabla z_t+ \\frac{\\nabla Z}{Z}\n",
    "$$\n",
    "\n",
    "因为有Z的存在，每次梯度下降时，都要计算完整的Z来计算 $\\nabla Z$ ，也就是每一个样本迭代一次的计算量就是 $\\mathscr{O}(n)$ 了，对于n比较大的情形，这是难以接受的，所以要寻求近似方案（huffman softmax是其中的一种，但是它写起来会比较复杂，而且huffman softmax的结果通常略差于普通的softmax，还有huffman softmax仅仅是训练快，但是如果预测时要找最大概率的标签，那么反而更加慢）。\n",
    "\n",
    "让我们进一步计算 $\\nabla L$ ：\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla L &= −\\nabla z_t+ \\frac{\\sum_ie^z_i \\nabla z_i}{Z} \\\\\n",
    "&= −\\nabla z_t+ \\frac{\\sum_ie^z_i}{Z}\\nabla z_i \\\\\n",
    "&= −\\nabla z_t+\\sum_ip_i\\nabla z_i \\\\\n",
    "&= −\\nabla z_t+ \\mathbb{E}(\\nabla z_i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "也就是说，最后的梯度由两项组成：一项是正确标签的梯度，一项是所有标签的梯度的均值，这两项反号，可以理解为这两项在“拉锯战”。计算量主要集中在第二项，因为要遍历所有才能算具体的均值。然而，均值本身就具有概率意义的，那么能不能直接就随机选取若干个来算这个梯度均值，而不用算全部梯度呢？如果可以的话，那每步更新的计算量就固定了，不会随着标签数的增大而快速增加。\n",
    "\n",
    "但如果这样做的话，需要按概率来随机选取标签，这也并不容易写。然而，有个更巧妙的方法是不用我们直接算梯度，我们可以直接对loss动手脚。所以这就导致了本文的loss：对于每个“样本-标签”对，随机选取nb_negative个标签，然后与原来的标签组成nb_negative+1个标签，直接在这nb_negative+1个标签中算softmax和交叉熵。选取这样的loss之后再去算梯度，就会发现自然就是按概率来选取的梯度均值了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Word2Vec import *\n",
    "# import pymongo\n",
    "# db = pymongo.MongoClient().travel.articles\n",
    "# class texts:\n",
    "#     def __iter__(self):\n",
    "#         for t in db.find().limit(30000):\n",
    "#             yield t['words']\n",
    "\n",
    "# wv = Word2Vec(texts(), model='cbow', nb_negative=16, shared_softmax=True, epochs=2) #建立并训练模型\n",
    "# wv.save_model('myvec') #保存到当前目录下的myvec文件夹\n",
    "\n",
    "# #训练完成后可以这样调用\n",
    "# wv = Word2Vec() #建立空模型\n",
    "# wv.load_model('myvec') #从当前目录下的myvec文件夹加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyter)",
   "language": "python",
   "name": "pycharm-34f9d306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
