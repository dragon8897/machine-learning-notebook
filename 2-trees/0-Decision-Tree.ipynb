{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "\n",
    "- error function:\n",
    "  $$\n",
    "  error = \\frac{\\text{#incorret num}}{\\text{#example num}}\n",
    "  $$\n",
    "\n",
    "\n",
    "### 构建决策树\n",
    "\n",
    "- 递归贪婪算法\n",
    "  1. 从空树开始\n",
    "  2. 选择一个特征开始分离数据\n",
    "  3. 对于每个子树\n",
    "    - 子树内为单一结果,则输出结果\n",
    "    - 否则,重复第 2 步\n",
    "  \n",
    "1. 如何选择特征?\n",
    "  - $\\hat y = $ 分类中的大部分数据\n",
    "  - 计算分类误差: $error = \\frac{\\hat y}{\\text{#total num}}$\n",
    "  - 对每个特征计算分类误差\n",
    "  - 选择分类误差最小的特征\n",
    "2. 何时终止?\n",
    "  - 特征使用完\n",
    "  - 所有数据都已分类\n",
    "3. 处理数值\n",
    "  - 先将特征值排序 {$v_1, v_2, ..., v_n$}\n",
    "  - 遍历排序, 令 $t_i = \\frac{v_i + v_{i+1}}{2}$, 计算分类误差 $h_j(x) >= t_i$\n",
    "  - 找到分类误差最小值的 $t_*$\n",
    "  \n",
    "处理过拟合问题:\n",
    "- 提前结束\n",
    "  - 限制树的深度\n",
    "  - 继续加深并不会降低分类误差时(短视而错过好的分支)\n",
    "  - 子树下的数据量过少,停止递归 (e.g $N_\\min = 10 ~ 100$)\n",
    "- 剪枝\n",
    "  - $L(T)$: 子节点数量\n",
    "  - 修改成本函数: $C(T) = Error(T) + \\lambda L(T)$ , 同时考量分类误差和树的复杂程度\n",
    "  - 步骤:\n",
    "    1. 选取待修剪的分支\n",
    "    2. 判断去除分支后的总成本\n",
    "    3. 如果总成本减小则修改该分支\n",
    "\n",
    "部分数据丢失处理:\n",
    "1. 放弃数据缺失的样本(缺失样本数据量小的时候适用)\n",
    "2. 放弃整个缺失的特征\n",
    "3. 猜测数据\n",
    "  - 最多数的类别\n",
    "  - 平均的数值\n",
    "4. **将 Unknown 数据添加到特征分支上(推荐)**\n",
    "  - 添加方法: 使分类误差最小\n",
    "  - 优点: 可解决猜测数据带来的偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义分类数据\n",
    "\n",
    "编号| 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 好瓜\n",
    "---|---|---|---|---|---|---|---\n",
    "1  |  青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 是 \n",
    "2  |  乌黑 | 蜡缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 是 \n",
    "3  |  乌黑 | 蜡缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 是 \n",
    "4  |  青绿 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 是 \n",
    "5  |  浅白 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 是 \n",
    "6  |  青绿 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 是 \n",
    "7  |  乌黑 | 稍蜷 | 浊响 | 稍糊 | 稍凹 | 软粘 | 是 \n",
    "8  |  乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 硬滑 | 是 \n",
    "9  |  乌黑 | 稍蜷 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 否 \n",
    "10 |  青绿 | 硬挺 | 清脆 | 清晰 | 平坦 | 软粘 | 否 \n",
    "11 |  洁白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 | 否 \n",
    "12 |  洁白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 软粘 | 否 \n",
    "13 |  青绿 | 稍蜷 | 浊响 | 稍糊 | 凹陷 | 硬滑 | 否 \n",
    "14 |  浅白 | 稍蜷 | 沉闷 | 稍糊 | 凹陷 | 硬情 | 否 \n",
    "15 |  乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 否 \n",
    "16 |  浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 硬滑 | 否 \n",
    "17 |  青绿 | 蜡缩 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 否 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([\n",
    "    [3, 4, 3, 3, 3, 2], \n",
    "    [1, 1, 1, 3, 3, 2], \n",
    "    [1, 1, 3, 3, 3, 2], \n",
    "    [3, 4, 1, 3, 3, 2], \n",
    "    [2, 4, 3, 3, 3, 2], \n",
    "    [3, 2, 3, 3, 1, 1], \n",
    "    [1, 2, 3, 1, 1, 1], \n",
    "    [1, 2, 3, 3, 1, 2], \n",
    "    [1, 2, 1, 1, 1, 2], \n",
    "    [3, 3, 2, 3, 2, 1], \n",
    "    [2, 3, 2, 2, 2, 2], \n",
    "    [2, 4, 3, 2, 2, 1], \n",
    "    [3, 2, 3, 1, 3, 2], \n",
    "    [2, 2, 1, 1, 3, 2], \n",
    "    [1, 2, 3, 3, 1, 1], \n",
    "    [2, 4, 3, 2, 2, 2], \n",
    "    [3, 1, 1, 1, 1, 2]\n",
    "], dtype=int)\n",
    "\n",
    "y = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据纯度计算\n",
    "\n",
    "1. 信息熵\n",
    "  - $p_k$: 第 k 类样本所占的比例($k = 1, 2, ..., |y|$)\n",
    "  - 越低越纯\n",
    "  $$\n",
    "  Ent(D) = -\\sum_{k=1}^{|y|}p_k\\log_2p_k\n",
    "  $$\n",
    "2. 信息增益\n",
    "  - 离散属性 a 有 V 个取值 $a^1, a^2, ..., a^V$\n",
    "  - 越高越纯\n",
    "  $$\n",
    "  Gain(D) = Ent(D) - \\sum_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)\n",
    "  $$\n",
    "2. 基尼指数: 数据集 D 中随机抽取 2 个样本不一致的概率\n",
    "  - 越低越纯\n",
    "  $$\n",
    "  Gini(D) = \\sum_{k=1}^{|y|}\\sum_{k'\\neq k}p_kp_{k'} = 1 - \\sum_{k=1}^{|y|}p_k^2\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / np.sum(hist)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "def gain(y, x):\n",
    "    n_features = x.shape[1]\n",
    "    gain = np.zeros(n_features) + entropy(y)\n",
    "    m = len(x)\n",
    "    for i in range(n_features):\n",
    "        v = np.bincount(x[:, i])\n",
    "        ent_v = np.sum([vv / m * entropy(y[x[:, i] == index]) for index, vv in enumerate(v) if vv > 0])\n",
    "        gain[i] -= ent_v\n",
    "    return gain\n",
    "\n",
    "def gini(y):\n",
    "    hist = np.bincount(y)\n",
    "    N = np.sum(hist)\n",
    "    return 1 - sum([(i / N) ** 2 for i in hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10812517 0.14419446 0.14078143 0.3805919  0.28915878 0.00604649]\n"
     ]
    }
   ],
   "source": [
    "print(gain(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义树结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right, rule):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = rule[0]\n",
    "        self.threshold = rule[1]\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, value):\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 具体算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier=True,\n",
    "        max_depth=None,\n",
    "        n_feats=None,\n",
    "        criterion=\"entropy\",\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A decision tree model for regression and classification problems.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        classifier : bool\n",
    "            Whether to treat target values as categorical (classifier =\n",
    "            True) or continuous (classifier = False). Default is True.\n",
    "        max_depth: int or None\n",
    "            The depth at which to stop growing the tree. If None, grow the tree\n",
    "            until all leaves are pure. Default is None.\n",
    "        n_feats : int\n",
    "            Specifies the number of features to sample on each split. If None,\n",
    "            use all features on each split. Default is None.\n",
    "        criterion : {'mse', 'entropy', 'gini'}\n",
    "            The error criterion to use when calculating splits.\n",
    "        seed : int or None\n",
    "            Seed for the random number generator. Default is None.\n",
    "        \"\"\"\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.depth = 0\n",
    "        self.root = None\n",
    "\n",
    "        self.n_feats = n_feats\n",
    "        self.criterion = criterion\n",
    "        self.classifier = classifier\n",
    "        self.max_depth = max_depth if max_depth else np.inf\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit a binary decision tree to a dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features\n",
    "        Y : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            An array of integer class labels for each example in `X` if\n",
    "            self.classifier = True, otherwise the set of target values for\n",
    "            each example in `X`.\n",
    "        \"\"\"\n",
    "        self.n_classes = max(Y) + 1 if self.classifier else None\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        self.root = self._grow(X, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained decision tree to classify or predict the examples in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preds : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            The integer class labels predicted for each example in `X` if\n",
    "            self.classifier = True, otherwise the predicted target values.\n",
    "        \"\"\"\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "\n",
    "    def predict_class_probs(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained decision tree to return the class probabilities for the\n",
    "        examples in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preds : :py:class:`ndarray <numpy.ndarray>` of shape `(N, n_classes)`\n",
    "            The class probabilities predicted for each example in `X`.\n",
    "        \"\"\"\n",
    "        assert self.classifier, \"`predict_class_probs` undefined for classifier = False\"\n",
    "        return np.array([self._traverse(x, self.root, prob=True) for x in X])\n",
    "\n",
    "    def _grow(self, X, Y):\n",
    "        # if all labels are the same, return a leaf\n",
    "        if len(np.unique(Y)) == 1:\n",
    "            if self.classifier:\n",
    "                prob = np.zeros(self.n_classes)\n",
    "                prob[Y[0]] = 1.0\n",
    "            return Leaf(prob) if self.classifier else Leaf(Y[0])\n",
    "\n",
    "        # if we have reached max_depth, return a leaf\n",
    "        if self.depth >= self.max_depth:\n",
    "            v = np.mean(Y, axis=0)\n",
    "            if self.classifier:\n",
    "                v = np.bincount(Y, minlength=self.n_classes) / len(Y)\n",
    "            return Leaf(v)\n",
    "\n",
    "        N, M = X.shape\n",
    "        self.depth += 1\n",
    "        feat_idxs = np.random.choice(M, self.n_feats, replace=False)\n",
    "\n",
    "        # greedily select the best split according to `criterion`\n",
    "        feat, thresh = self._segment(X, Y, feat_idxs)\n",
    "        l = np.argwhere(X[:, feat] <= thresh).flatten()\n",
    "        r = np.argwhere(X[:, feat] > thresh).flatten()\n",
    "\n",
    "        # grow the children that result from the split\n",
    "        left = self._grow(X[l, :], Y[l])\n",
    "        right = self._grow(X[r, :], Y[r])\n",
    "        return Node(left, right, (feat, thresh))\n",
    "\n",
    "    def _segment(self, X, Y, feat_idxs):\n",
    "        \"\"\"\n",
    "        Find the optimal split rule (feature index and split threshold) for the\n",
    "        data according to `self.criterion`.\n",
    "        \"\"\"\n",
    "        best_gain = -np.inf\n",
    "        split_idx, split_thresh = None, None\n",
    "        for i in feat_idxs:\n",
    "            vals = X[:, i]\n",
    "            levels = np.unique(vals)\n",
    "            if len(levels) > 1:\n",
    "                thresholds = (levels[:-1] + levels[1:]) / 2\n",
    "            else:\n",
    "                thresholds = levels\n",
    "            gains = np.array([self._impurity_gain(Y, t, vals) for t in thresholds])\n",
    "\n",
    "            if gains.max() > best_gain:\n",
    "                split_idx = i\n",
    "                best_gain = gains.max()\n",
    "                split_thresh = thresholds[gains.argmax()]\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _impurity_gain(self, Y, split_thresh, feat_values):\n",
    "        \"\"\"\n",
    "        Compute the impurity gain associated with a given split.\n",
    "\n",
    "        IG(split) = loss(parent) - weighted_avg[loss(left_child), loss(right_child)]\n",
    "        \"\"\"\n",
    "        if self.criterion == \"entropy\":\n",
    "            loss = entropy\n",
    "        else:\n",
    "            loss = gini\n",
    "\n",
    "        parent_loss = loss(Y)\n",
    "\n",
    "        # generate split\n",
    "        left = np.argwhere(feat_values <= split_thresh).flatten()\n",
    "        right = np.argwhere(feat_values > split_thresh).flatten()\n",
    "\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "\n",
    "        # compute the weighted avg. of the loss for the children\n",
    "        n = len(Y)\n",
    "        n_l, n_r = len(left), len(right)\n",
    "        e_l, e_r = loss(Y[left]), loss(Y[right])\n",
    "        child_loss = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # impurity gain is difference in loss before vs. after split\n",
    "        ig = parent_loss - child_loss\n",
    "        return ig\n",
    "\n",
    "    def _traverse(self, X, node, prob=False):\n",
    "        if isinstance(node, Leaf):\n",
    "            if self.classifier:\n",
    "                return node.value if prob else node.value.argmax()\n",
    "            return node.value\n",
    "        if X[node.feature] <= node.threshold:\n",
    "            return self._traverse(X, node.left, prob)\n",
    "        return self._traverse(X, node.right, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree()\n",
    "dt.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(dt.predict(np.array([\n",
    "    [2, 4, 3, 3, 3, 2],\n",
    "    [1, 2, 3, 3, 1, 1],\n",
    "])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyter)",
   "language": "python",
   "name": "pycharm-34f9d306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
