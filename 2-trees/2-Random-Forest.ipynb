{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机森林\n",
    "\n",
    "### Bagging \n",
    "\n",
    "从数据集中随机取一个样本,然后把样本放回数据集,重复 N次,既可得 N 个样本的采样集\n",
    "则: 数据集中有 63.2% 的样本出现在采样集中,剩下的 36.8% 作为验证集\n",
    "\n",
    "### 步骤:\n",
    "1. 创建随机森林\n",
    "  - 使用 Bagging 选择 N 个样本\n",
    "  - 随机从 M 个属性中选取 m 个属性 ($m \\ll M$) 来作为分裂属性(不剪枝)\n",
    "  - 按照以上 2 步建立大量决策树, 则构成随机森林\n",
    "2. 使用验证集校验随机森林\n",
    "3. 少于迭代次数则重复 1 \n",
    "\n",
    "\n",
    "### 数据缺失的处理\n",
    "\n",
    "1. 训练数据缺失\n",
    "  - 先使用属性的均值占位\n",
    "  - 创建邻近矩阵记录邻近的数据(决策树中路径一致则 +1)\n",
    "  - 正规化邻近矩阵\n",
    "  - 更新缺失数据 = 使用邻近矩阵中的权重 * 数据\n",
    "2. 预测数据缺失\n",
    "  - 先猜测结果\n",
    "  - 使用属性的均值填写缺失数据\n",
    "  - 分别进入随机森林预测, 分类误差少的结果胜利\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义[决策树](./0-Decision-Tree.ipynb#%E5%86%B3%E7%AD%96%E6%A0%91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, left, right, rule):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = rule[0]\n",
    "        self.threshold = rule[1]\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier=True,\n",
    "        max_depth=None,\n",
    "        n_feats=None,\n",
    "        criterion=\"entropy\",\n",
    "        seed=None,\n",
    "    ):\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.depth = 0\n",
    "        self.root = None\n",
    "\n",
    "        self.n_feats = n_feats\n",
    "        self.criterion = criterion\n",
    "        self.classifier = classifier\n",
    "        self.max_depth = max_depth if max_depth else np.inf\n",
    "\n",
    "        if not classifier and criterion in [\"gini\", \"entropy\"]:\n",
    "            raise ValueError(\n",
    "                \"{} is a valid criterion only when classifier = True.\".format(criterion)\n",
    "            )\n",
    "        if classifier and criterion == \"mse\":\n",
    "            raise ValueError(\"`mse` is a valid criterion only when classifier = False.\")\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.n_classes = max(Y) + 1 if self.classifier else None\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        self.root = self._grow(X, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "\n",
    "    def predict_class_probs(self, X):\n",
    "        assert self.classifier, \"`predict_class_probs` undefined for classifier = False\"\n",
    "        return np.array([self._traverse(x, self.root, prob=True) for x in X])\n",
    "\n",
    "    def _grow(self, X, Y):\n",
    "        # if all labels are the same, return a leaf\n",
    "        if len(set(Y)) == 1:\n",
    "            if self.classifier:\n",
    "                prob = np.zeros(self.n_classes)\n",
    "                prob[Y[0]] = 1.0\n",
    "            return Leaf(prob) if self.classifier else Leaf(Y[0])\n",
    "\n",
    "        # if we have reached max_depth, return a leaf\n",
    "        if self.depth >= self.max_depth:\n",
    "            v = np.mean(Y, axis=0)\n",
    "            if self.classifier:\n",
    "                v = np.bincount(Y, minlength=self.n_classes) / len(Y)\n",
    "            return Leaf(v)\n",
    "\n",
    "        N, M = X.shape\n",
    "        self.depth += 1\n",
    "        feat_idxs = np.random.choice(M, self.n_feats, replace=False)\n",
    "\n",
    "        # greedily select the best split according to `criterion`\n",
    "        feat, thresh = self._segment(X, Y, feat_idxs)\n",
    "        l = np.argwhere(X[:, feat] <= thresh).flatten()\n",
    "        r = np.argwhere(X[:, feat] > thresh).flatten()\n",
    "\n",
    "        # grow the children that result from the split\n",
    "        left = self._grow(X[l, :], Y[l])\n",
    "        right = self._grow(X[r, :], Y[r])\n",
    "        return Node(left, right, (feat, thresh))\n",
    "\n",
    "    def _segment(self, X, Y, feat_idxs):\n",
    "        best_gain = -np.inf\n",
    "        split_idx, split_thresh = None, None\n",
    "        for i in feat_idxs:\n",
    "            vals = X[:, i]\n",
    "            levels = np.unique(vals)\n",
    "            thresholds = (levels[:-1] + levels[1:]) / 2\n",
    "            gains = np.array([self._impurity_gain(Y, t, vals) for t in thresholds])\n",
    "\n",
    "            if gains.max() > best_gain:\n",
    "                split_idx = i\n",
    "                best_gain = gains.max()\n",
    "                split_thresh = thresholds[gains.argmax()]\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _impurity_gain(self, Y, split_thresh, feat_values):\n",
    "        if self.criterion == \"entropy\":\n",
    "            loss = entropy\n",
    "        elif self.criterion == \"gini\":\n",
    "            loss = gini\n",
    "        elif self.criterion == \"mse\":\n",
    "            loss = mse\n",
    "\n",
    "        parent_loss = loss(Y)\n",
    "\n",
    "        # generate split\n",
    "        left = np.argwhere(feat_values <= split_thresh).flatten()\n",
    "        right = np.argwhere(feat_values > split_thresh).flatten()\n",
    "\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "\n",
    "        # compute the weighted avg. of the loss for the children\n",
    "        n = len(Y)\n",
    "        n_l, n_r = len(left), len(right)\n",
    "        e_l, e_r = loss(Y[left]), loss(Y[right])\n",
    "        child_loss = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # impurity gain is difference in loss before vs. after split\n",
    "        ig = parent_loss - child_loss\n",
    "        return ig\n",
    "\n",
    "    def _traverse(self, X, node, prob=False):\n",
    "        if isinstance(node, Leaf):\n",
    "            if self.classifier:\n",
    "                return node.value if prob else node.value.argmax()\n",
    "            return node.value\n",
    "        if X[node.feature] <= node.threshold:\n",
    "            return self._traverse(X, node.left, prob)\n",
    "        return self._traverse(X, node.right, prob)\n",
    "\n",
    "\n",
    "def mse(y):\n",
    "    return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / np.sum(hist)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    hist = np.bincount(y)\n",
    "    N = np.sum(hist)\n",
    "    return 1 - sum([(i / N) ** 2 for i in hist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X, Y):\n",
    "    N, M = X.shape\n",
    "    idxs = np.random.choice(N, N, replace=True)\n",
    "    return X[idxs], Y[idxs]\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(\n",
    "        self, n_trees, max_depth, n_feats, classifier=True, criterion=\"entropy\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        An ensemble (forest) of decision trees where each split is calculated\n",
    "        using a random subset of the features in the input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trees : int\n",
    "            The number of individual decision trees to use within the ensemble.\n",
    "        max_depth: int or None\n",
    "            The depth at which to stop growing each decision tree. If None,\n",
    "            grow each tree until the leaf nodes are pure.\n",
    "        n_feats : int\n",
    "            The number of features to sample on each split.\n",
    "        classifier : bool\n",
    "            Whether `Y` contains class labels or real-valued targets. Default\n",
    "            is True.\n",
    "        criterion : {'entropy', 'gini', 'mse'}\n",
    "            The error criterion to use when calculating splits for each weak\n",
    "            learner. When ``classifier = False``, valid entries are {'mse'}.\n",
    "            When ``classifier = True``, valid entries are {'entropy', 'gini'}.\n",
    "            Default is 'entropy'.\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        self.n_trees = n_trees\n",
    "        self.n_feats = n_feats\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Create `n_trees`-worth of bootstrapped samples from the training data\n",
    "        and use each to fit a separate decision tree.\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            X_samp, Y_samp = bootstrap_sample(X, Y)\n",
    "            tree = DecisionTree(\n",
    "                n_feats=self.n_feats,\n",
    "                max_depth=self.max_depth,\n",
    "                criterion=self.criterion,\n",
    "                classifier=self.classifier,\n",
    "            )\n",
    "            tree.fit(X_samp, Y_samp)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target value for each entry in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            Model predictions for each entry in `X`.\n",
    "        \"\"\"\n",
    "        tree_preds = np.array([[t._traverse(x, t.root) for x in X] for t in self.trees])\n",
    "        return self._vote(tree_preds)\n",
    "\n",
    "    def _vote(self, predictions):\n",
    "        \"\"\"\n",
    "        Return the aggregated prediction across all trees in the RF for each problem.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        predictions : :py:class:`ndarray <numpy.ndarray>` of shape `(n_trees, N)`\n",
    "            The array of predictions from each decision tree in the RF for each\n",
    "            of the `N` problems in `X`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            If classifier is True, the class label predicted by the majority of\n",
    "            the decision trees for each problem in `X`. If classifier is False,\n",
    "            the average prediction across decision trees on each problem.\n",
    "        \"\"\"\n",
    "        if self.classifier:\n",
    "            out = [np.bincount(x).argmax() for x in predictions.T]\n",
    "        else:\n",
    "            out = [np.mean(x) for x in predictions.T]\n",
    "        return np.array(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyter)",
   "language": "python",
   "name": "pycharm-34f9d306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
