{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "每次迭代添加一个新树来填补上上次产生的残差值,来达到贴近真实值的目的.\n",
    "\n",
    "### 数学原理\n",
    "\n",
    "- 目标: $Obj^{(t)} = \\sum_{i=1}^nl(y_i, \\hat y_i^{(t)}) + \\sum_{i=1}^t\\Omega(f_i) $ 其中: $\\hat y_i^{(t)} = \\hat y_i^{(t-1)} + f_t(x_i)$\n",
    "  - 利用二阶泰勒展开式: $f(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac12f''(x)\\Delta x^2$\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  &令: \\\\\n",
    "  &g_i = \\partial_{\\hat y^{(t-1)}}l(y_i, \\hat y_i^{(t-1)}) \\\\\n",
    "  &h_i = \\partial^2_{\\hat y^{(t-1)}}l(y_i, \\hat y_i^{(t-1)}) \\\\\n",
    "  &则: \\\\\n",
    "  &Obj^{(t)} \\approx \\sum_{i=1}^n\\left[l(y_i, \\hat y_i^{(t-1)}) + g_i f_t(x_i) + \\frac12h_if_t^2(x_i)\\right] + \\Omega(f_t) \\\\\n",
    "  &去除常数项 \\\\\n",
    "  &Obj^{(t)} \\approx \\sum_{i=1}^n\\left[ g_i f_t(x_i) + \\frac12h_if_t^2(x_i)\\right] + \\Omega(f_t) \\\\\n",
    "  &定义 f_t(x): \\\\\n",
    "  &f_t(x) = w_{q(x)} \\quad (w \\in \\mathbb{R}^T: 叶节点的权重, q(x): 数据对应叶节点的 index) \\\\\n",
    "  &定义正则项: \\\\\n",
    "  &\\Omega(f_t) = \\gamma T + \\frac12\\lambda\\sum_{j=1}^Tw_j^2 \\\\\n",
    "  &令: \\\\\n",
    "  &I_j = \\{i\\;|\\;q(x_i) = j\\} \\\\\n",
    "  &则: \\\\\n",
    "  &Obj^{(t)} \\approx \\sum_{j=1}^T\\left[(\\sum_{i\\in I_j}g_i)w_j + \\frac12(\\sum_{i\\in I_j}h_i + \\lambda)w_j^2\\right] + \\gamma T \\\\\n",
    "  &令: \\\\\n",
    "  &G_j = \\sum_{i \\in I_j}g_i \\\\\n",
    "  &H_j = \\sum_{i \\in I_j}h_i \\\\\n",
    "  &则: \\\\\n",
    "  &Obj^{(t)} \\approx \\sum_{j=1}^T\\left[G_jw_j + \\frac12(H_j + \\lambda)w_j^2\\right] + \\gamma T \\\\\n",
    "  &求最小值,得: \\\\\n",
    "  &w_j^* = -\\frac{G_j}{H_j + \\lambda} \\\\\n",
    "  &Obj^* = -\\frac12\\sum_{j=1}^T\\frac{G_j^2}{H_j + \\lambda} + \\gamma T \\\\\n",
    "  &对于一个节点分裂为两个叶节点时,得: \\\\\n",
    "  &Gain = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\\right] - \\gamma \\\\\n",
    "  &结论: 使得 Gain 增加最多, 则是最佳分裂 \\\\\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "特征分裂算法:\n",
    "1. 每个节点, 列举所有的特征\n",
    "    1. 每个特征下数据排序\n",
    "    2. 使用线性扫描的方法,决定最佳分裂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义[决策树](./0-Decision-Tree.ipynb#%E5%86%B3%E7%AD%96%E6%A0%91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, left, right, rule):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = rule[0]\n",
    "        self.threshold = rule[1]\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class XGBoostDecisionTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier=True,\n",
    "        max_depth=None,\n",
    "        seed=None,\n",
    "    ):\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.depth = 0\n",
    "        self.root = None\n",
    "\n",
    "        self.reg_lambda = 1.\n",
    "        self.classifier = classifier\n",
    "        self.max_depth = max_depth if max_depth else np.inf\n",
    "        self.loss = LogisticRegression() if classifier else LinearSquareLoss()\n",
    "\n",
    "\n",
    "    def fit(self, X, Y, Y_pred):\n",
    "        self.n_classes = max(Y) + 1 if self.classifier else None\n",
    "        self.n_feats = X.shape[1] \n",
    "        self.root = self._grow(X, Y, Y_pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "\n",
    "    def predict_class_probs(self, X):\n",
    "        assert self.classifier, \"`predict_class_probs` undefined for classifier = False\"\n",
    "        return np.array([self._traverse(x, self.root, prob=True) for x in X])\n",
    "\n",
    "    def _grow(self, X, Y, Y_pred):\n",
    "        # if all labels are the same, return a leaf\n",
    "        if len(set(Y)) == 1:\n",
    "            v = self._compute_weight(Y, Y_pred)\n",
    "            return Leaf(v) \n",
    "\n",
    "        # if we have reached max_depth, return a leaf\n",
    "        if self.depth >= self.max_depth:\n",
    "            v = self._compute_weight(Y, Y_pred)\n",
    "            return Leaf(v) \n",
    "\n",
    "        N, M = X.shape\n",
    "        self.depth += 1\n",
    "        feat_idxs = np.random.choice(M, self.n_feats, replace=False)\n",
    "\n",
    "        feat, thresh = self._segment(X, Y, Y_pred, feat_idxs)\n",
    "        l = np.argwhere(X[:, feat] <= thresh).flatten()\n",
    "        r = np.argwhere(X[:, feat] > thresh).flatten()\n",
    "\n",
    "        # grow the children that result from the split\n",
    "        left = self._grow(X[l, :], Y[l], Y_pred[l])\n",
    "        right = self._grow(X[r, :], Y[r], Y_pred[r])\n",
    "        return Node(left, right, (feat, thresh))\n",
    "\n",
    "    def _segment(self, X, Y, Y_pred, feat_idxs):\n",
    "        best_gain = -np.inf\n",
    "        split_idx, split_thresh = None, None\n",
    "        for i in feat_idxs:\n",
    "            vals = X[:, i]\n",
    "            levels = np.unique(vals)\n",
    "            thresholds = (levels[:-1] + levels[1:]) / 2\n",
    "            gains = np.array([self._impurity_gain(Y, Y_pred, t, vals) for t in thresholds])\n",
    "\n",
    "            if gains.max() > best_gain:\n",
    "                split_idx = i\n",
    "                best_gain = gains.max()\n",
    "                split_thresh = thresholds[gains.argmax()]\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "    \n",
    "    def _gain(self, y, y_pred):\n",
    "        nominator = np.power((self.loss.gradient(y, y_pred)).sum(), 2)\n",
    "        denominator = self.loss.hess(y, y_pred).sum()\n",
    "        return 0.5 * (nominator / denominator + self.reg_lambda)\n",
    "    \n",
    "    def _compute_weight(self, y, y_pred):\n",
    "        nominator = self.loss.gradient(y, y_pred).sum()\n",
    "        denominator = self.loss.hess(y, y_pred).sum()\n",
    "        return -nominator / (denominator + self.reg_lambda)\n",
    "\n",
    "    def _impurity_gain(self, y, y_pred, split_thresh, feat_values):\n",
    "        # generate split\n",
    "        left = np.argwhere(feat_values <= split_thresh).flatten()\n",
    "        right = np.argwhere(feat_values > split_thresh).flatten()\n",
    "\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "\n",
    "        left_gain = self._gain(y[left], y_pred[left])\n",
    "        right_gain = self._gain(y[right], y_pred[right])\n",
    "        gain = self._gain(y, y_pred)\n",
    "        return left_gain + right_gain - gain\n",
    "\n",
    "    def _traverse(self, X, node, prob=False):\n",
    "        if isinstance(node, Leaf):\n",
    "            return node.value\n",
    "        if X[node.feature] <= node.threshold:\n",
    "            return self._traverse(X, node.left, prob)\n",
    "        return self._traverse(X, node.right, prob)\n",
    "\n",
    "class LinearSquareLoss():\n",
    "    \"\"\"Least squares loss\"\"\"\n",
    "\n",
    "    def gradient(self, label, pred):\n",
    "        return pred - label\n",
    "\n",
    "    def hess(self, label, pred):\n",
    "        return np.ones_like(pred)\n",
    "    \n",
    "\n",
    "class LogisticRegression():\n",
    "    \"\"\"logistic regression loss\"\"\"\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1. / (1. + np.exp(-z))\n",
    "\n",
    "    def gradient(self, label, pred):\n",
    "        pred = self._sigmoid(pred)\n",
    "        return pred - label\n",
    "\n",
    "    def hess(self, label, pred):\n",
    "        pred = self._sigmoid(pred)\n",
    "        eps = np.finfo(float).eps\n",
    "        return np.maximum(pred * (1. - pred), eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost(object):\n",
    "    \"\"\"The XGBoost classifier.\n",
    "\n",
    "    Reference: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators: int\n",
    "        The number of classification trees that are used.\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        classifier=True,\n",
    "        n_estimators=200, \n",
    "        learning_rate=0.01, \n",
    "        max_depth=2\n",
    "    ):\n",
    "        self.n_estimators = n_estimators  # Number of trees\n",
    "        self.learning_rate = learning_rate  # Step size for weight update\n",
    "        self.max_depth = max_depth  # Maximum depth for tree\n",
    "\n",
    "        # Initialize regression trees\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            tree = XGBoostDecisionTree(classifier=classifier, max_depth=self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.zeros_like(y)\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = self.trees[i]\n",
    "            tree.fit(X, y, y_pred)\n",
    "            update_pred = tree.predict(X)\n",
    "            y_pred += update_pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = None\n",
    "        # Make predictions\n",
    "        for tree in self.trees:\n",
    "            # Estimate gradient and update prediction\n",
    "            update_pred = tree.predict(X)\n",
    "            if y_pred is None:\n",
    "                y_pred = np.zeros_like(update_pred)\n",
    "            y_pred += update_pred\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyter)",
   "language": "python",
   "name": "pycharm-34f9d306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
