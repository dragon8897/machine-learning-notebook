{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "- 输出: $\\hat y$ (-1 或 1)\n",
    "- 输入: x\n",
    "- 学习组合 model:\n",
    "  - 分类器: $f_1(x), f_2(x), ..., f_T(x)$\n",
    "  - 权重: $w_1, w_2, ..., w_T$\n",
    "- 预测:\n",
    "  $$\n",
    "  \\hat y = sign(\\sum_{t=1}^Tw_tf_t(x))\n",
    "  $$\n",
    "\n",
    "### AdaBoost 算法\n",
    "- 每个数据的权重: $\\alpha_i = \\frac1N$\n",
    "  - 当 $f_t(x_i)$ 出错时, 增加 $\\alpha_i$\n",
    "  - 权重误差:\n",
    "    $$\n",
    "    weightError = \\sum_{i=1, f_t(x_i) \\neq y_i}^N\\alpha_i\n",
    "    $$\n",
    "- For t = 1, ..., T\n",
    "  - 学习 $f_t(x)$: 选取分类误差最小的特征\n",
    "  - 计算权重 $w_t$\n",
    "    $$\n",
    "    w_t = \\frac12\\ln\\left(\\frac{1 - weightError(f_t)}{weightError(f_t)}\\right)\n",
    "    $$\n",
    "  - 重新计算 $\\alpha_i$\n",
    "    $$\n",
    "    \\alpha_i = \\begin{cases}\\alpha_i e^{-w_t}, & f_t(x_i) = y_i\\\\\\alpha_i e^{w_t}, & f_t(x_i) \\neq y_i\\end{cases}\n",
    "    $$\n",
    "  - 正规化 $\\alpha_i$ (避免数据爆炸或数据消失)\n",
    "    $$\n",
    "    \\alpha_i = \\frac{\\alpha_i}{\\sum_{j=1}^N\\alpha_j}\n",
    "    $$\n",
    "- 预测:\n",
    "  $$\n",
    "  \\hat y = sign(\\sum_{t=1}^Tw_tf_t(x))\n",
    "  $$\n",
    "\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "**构建的树叶节点数量为: 8~32**\n",
    "\n",
    "- 输入: \n",
    "  - 数据集 {$(x_1, y_1),(x_2, y_2), ...,(x_n, y_n)$} \n",
    "  - 可微损失函数 $L(y_i, F(x))$\n",
    "  - 学习速率: $\\nu$\n",
    "- 初始化常数变量: \n",
    "  $$\n",
    "  F_0(x) = \\mathop{\\arg\\min}_{\\gamma}\\sum_{i=1}^nL(y_i, \\gamma)\n",
    "  $$\n",
    "- for m = 1, M(生成树的数量 $\\geq$ 100)\n",
    "  - 计算 $r_{i, m} = - \\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x) = F_{m-1}(x)}$ for i = 1, 2, ..., n\n",
    "  - 根据 {$(x_1, r_{1, m}), (x_2, r_{2, m}), ..., (x_n, r_{n, m})$} 构建回归树, 叶节点为: $R_{j,m} = \\{F_{m-1}(x),...\\}\\; (j \\in J_m:\\text{叶节点的总数量})$\n",
    "  - $\\gamma_{j,m} = \\mathop{\\arg\\min}_{\\gamma}\\sum_{x_i \\in R_{i, j}}L(y_i, F_{m-1}(x_i) + \\gamma)$\n",
    "  - 更新 $F_m(x) = F_{m-1}(x) + \\nu \\sum_{j=1}^{J_m}\\gamma_{j,m}I(x\\in R_{j, m})$\n",
    "- 输出 $F_M(x)$\n",
    "\n",
    "详细问题的区别:\n",
    "- 回归问题:\n",
    "  - 学习速率: $\\nu = 0.1$\n",
    "  - 损失函数: $L(y_i, F(x_i)) = \\frac12\\sum_{i=1}^N(y_i - F(x_i))^2$\n",
    "  - 计算 $\\gamma_{j,m} = \\frac{\\sum R_{j,m}}{\\#R_{j,m}}$\n",
    "- 分类问题:\n",
    "  - 学习速率: $\\nu = 0.8$\n",
    "  - 损失函数:  $L(y_i, F(x_i)) = -\\sum_{i=1}^N(y_i\\log(F(x_i)) + (1-y_i)\\log(1-F(x_i)))$\n",
    "  - 计算 $\\gamma_{j,m}$: 需要利用二项式的泰勒展开式\n",
    "    $$\n",
    "    L(y_i, F_{m-1}(x_i) + \\gamma) \\approx L(y_i, F_{m-1}(x_i)) + \\frac{\\partial (y_i, F_{m-1}(x_i))}{\\partial F()}\\gamma + \\frac12\\frac{\\partial^2 (y_i, F_{m-1}(x_i))}{\\partial F()^2}\\gamma^2\n",
    "    $$\n",
    "    简化后:\n",
    "    $$\n",
    "    \\gamma_{j,m} = \\frac{\\sum R_{j, m}}{\\sum_{x_i \\in R_{j, m}} F_{m-1}(x_i) (1 - F_{m-1}(x_i))}\n",
    "    $$\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "每次迭代添加一个新树来填补上上次产生的残差值,来达到贴近真实值的目的.\n",
    "\n",
    "- 目标: $Obj^{(t)} = \\sum_{i=1}^nl(y_i, \\hat y_i^{(t)}) + \\sum_{i=1}^t\\Omega(f_i) $ 其中: $\\hat y_i^{(t)} = \\hat y_i^{(t-1)} + f_t(x_i)$\n",
    "  - 利用二阶泰勒展开式: $f(x + \\Delta x) \\approx f(x) + f'(x)\\Delta x + \\frac12f''(x)\\Delta x^2$\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  &令: \\\\\n",
    "  &g_i = \\partial_{\\hat y^{(t-1)}}l(y_i, \\hat y_i^{(t-1)}) \\\\\n",
    "  &h_i = \\partial^2_{\\hat y^{(t-1)}}l(y_i, \\hat y_i^{(t-1)}) \\\\\n",
    "  &则: \\\\\n",
    "  &Obj^{(t)} \\approx \\sum_{i=1}^n\\left[l(y_i, \\hat y_i^{(t-1)}) + g_i f_t(x_i) + \\frac12h_if_t^2(x_i)\\right] + \\Omega(f_t) \\\\\n",
    "  &去除常数项 \\\\\n",
    "  &Obj^{(t)} \\approx \\sum_{i=1}^n\\left[ g_i f_t(x_i) + \\frac12h_if_t^2(x_i)\\right] + \\Omega(f_t) \\\\\n",
    "  &定义 f_t(x): \\\\\n",
    "  &f_t(x) = w_{q(x)} \\quad (w \\in \\mathbb{R}^T: 叶节点的权重, q(x): 数据对应叶节点的 index) \\\\\n",
    "  &定义正则项: \\\\\n",
    "  &\\Omega(f_t) = \\gamma T + \\frac12\\lambda\\sum_{j=1}^Tw_j^2 \\\\\n",
    "  &令: \\\\\n",
    "  &I_j = \\{i\\;|\\;q(x_i) = j\\} \\\\\n",
    "  &则: \\\\\n",
    "  &Obj^{(t)} \\approx \\sum_{j=1}^T\\left[(\\sum_{i\\in I_j}g_i)w_j + \\frac12(\\sum_{i\\in I_j}h_i + \\lambda)w_j^2\\right] + \\gamma T \\\\\n",
    "  &令: \\\\\n",
    "  &G_j = \\sum_{i \\in I_j}g_i \\\\\n",
    "  &H_j = \\sum_{i \\in I_j}h_i \\\\\n",
    "  &则: \\\\\n",
    "  &Obj^{(t)} \\approx \\sum_{j=1}^T\\left[G_jw_j + \\frac12(H_j + \\lambda)w_j^2\\right] + \\gamma T \\\\\n",
    "  &求最小值,得: \\\\\n",
    "  &w_j^* = -\\frac{G_j}{H_j + \\lambda} \\\\\n",
    "  &Obj^* = -\\frac12\\sum_{j=1}^T\\frac{G_j^2}{H_j + \\lambda} + \\gamma T \\\\\n",
    "  &对于一个节点分裂为两个叶节点时,得: \\\\\n",
    "  &Gain = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\\right] - \\gamma \\\\\n",
    "  &结论: 使得 Gain 增加最多, 则是最佳分裂 \\\\\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "特征分裂算法:\n",
    "1. 每个节点, 列举所有的特征\n",
    "    1. 每个特征下数据排序\n",
    "    2. 使用线性扫描的方法,决定最佳分裂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyter)",
   "language": "python",
   "name": "pycharm-34f9d306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
