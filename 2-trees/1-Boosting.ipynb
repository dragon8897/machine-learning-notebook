{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "- 输出: $\\hat y$ (-1 或 1)\n",
    "- 输入: x\n",
    "- 学习组合 model:\n",
    "  - 分类器: $f_1(x), f_2(x), ..., f_T(x)$\n",
    "  - 权重: $w_1, w_2, ..., w_T$\n",
    "- 预测:\n",
    "  $$\n",
    "  \\hat y = sign(\\sum_{t=1}^Tw_tf_t(x))\n",
    "  $$\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "**构建的树叶节点数量为: 8~32**\n",
    "\n",
    "- 输入: \n",
    "  - 数据集 {$(x_1, y_1),(x_2, y_2), ...,(x_n, y_n)$} \n",
    "  - 可微损失函数 $L(y_i, F(x))$\n",
    "  - 学习速率: $\\nu$\n",
    "- 初始化常数变量: \n",
    "  $$\n",
    "  F_0(x) = \\mathop{\\arg\\min}_{\\gamma}\\sum_{i=1}^nL(y_i, \\gamma)\n",
    "  $$\n",
    "- for m = 1, M(生成树的数量 $\\geq$ 100)\n",
    "  - 计算 $r_{i, m} = - \\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x) = F_{m-1}(x)}$ for i = 1, 2, ..., n\n",
    "  - 根据 {$(x_1, r_{1, m}), (x_2, r_{2, m}), ..., (x_n, r_{n, m})$} 构建回归树, 叶节点为: $R_{j,m} = \\{F_{m-1}(x),...\\}\\; (j \\in J_m:\\text{叶节点的总数量})$\n",
    "  - $\\gamma_{j,m} = \\mathop{\\arg\\min}_{\\gamma}\\sum_{x_i \\in R_{i, j}}L(y_i, F_{m-1}(x_i) + \\gamma)$\n",
    "  - 更新 $F_m(x) = F_{m-1}(x) + \\nu \\sum_{j=1}^{J_m}\\gamma_{j,m}I(x\\in R_{j, m})$\n",
    "- 输出 $F_M(x)$\n",
    "\n",
    "详细问题的区别:\n",
    "- 回归问题:\n",
    "  - 学习速率: $\\nu = 0.1$\n",
    "  - 损失函数: $L(y_i, F(x_i)) = \\frac12\\sum_{i=1}^N(y_i - F(x_i))^2$\n",
    "  - 计算 $\\gamma_{j,m} = \\frac{\\sum R_{j,m}}{\\#R_{j,m}}$\n",
    "- 分类问题:\n",
    "  - 学习速率: $\\nu = 0.8$\n",
    "  - 损失函数:  $L(y_i, F(x_i)) = -\\sum_{i=1}^N(y_i\\log(F(x_i)) + (1-y_i)\\log(1-F(x_i)))$\n",
    "  - 计算 $\\gamma_{j,m}$: 需要利用二项式的泰勒展开式\n",
    "    $$\n",
    "    L(y_i, F_{m-1}(x_i) + \\gamma) \\approx L(y_i, F_{m-1}(x_i)) + \\frac{\\partial (y_i, F_{m-1}(x_i))}{\\partial F()}\\gamma + \\frac12\\frac{\\partial^2 (y_i, F_{m-1}(x_i))}{\\partial F()^2}\\gamma^2\n",
    "    $$\n",
    "    简化后:\n",
    "    $$\n",
    "    \\gamma_{j,m} = \\frac{\\sum R_{j, m}}{\\sum_{x_i \\in R_{j, m}} F_{m-1}(x_i) (1 - F_{m-1}(x_i))}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义[决策树](./0-Decision-Tree.ipynb#%E5%86%B3%E7%AD%96%E6%A0%91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, left, right, rule):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = rule[0]\n",
    "        self.threshold = rule[1]\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier=True,\n",
    "        max_depth=None,\n",
    "        n_feats=None,\n",
    "        criterion=\"entropy\",\n",
    "        seed=None,\n",
    "    ):\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.depth = 0\n",
    "        self.root = None\n",
    "\n",
    "        self.n_feats = n_feats\n",
    "        self.criterion = criterion\n",
    "        self.classifier = classifier\n",
    "        self.max_depth = max_depth if max_depth else np.inf\n",
    "\n",
    "        if not classifier and criterion in [\"gini\", \"entropy\"]:\n",
    "            raise ValueError(\n",
    "                \"{} is a valid criterion only when classifier = True.\".format(criterion)\n",
    "            )\n",
    "        if classifier and criterion == \"mse\":\n",
    "            raise ValueError(\"`mse` is a valid criterion only when classifier = False.\")\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.n_classes = max(Y) + 1 if self.classifier else None\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        self.root = self._grow(X, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "\n",
    "    def predict_class_probs(self, X):\n",
    "        assert self.classifier, \"`predict_class_probs` undefined for classifier = False\"\n",
    "        return np.array([self._traverse(x, self.root, prob=True) for x in X])\n",
    "\n",
    "    def _grow(self, X, Y):\n",
    "        # if all labels are the same, return a leaf\n",
    "        if len(set(Y)) == 1:\n",
    "            if self.classifier:\n",
    "                prob = np.zeros(self.n_classes)\n",
    "                prob[Y[0]] = 1.0\n",
    "            return Leaf(prob) if self.classifier else Leaf(Y[0])\n",
    "\n",
    "        # if we have reached max_depth, return a leaf\n",
    "        if self.depth >= self.max_depth:\n",
    "            v = np.mean(Y, axis=0)\n",
    "            if self.classifier:\n",
    "                v = np.bincount(Y, minlength=self.n_classes) / len(Y)\n",
    "            return Leaf(v)\n",
    "\n",
    "        N, M = X.shape\n",
    "        self.depth += 1\n",
    "        feat_idxs = np.random.choice(M, self.n_feats, replace=False)\n",
    "\n",
    "        # greedily select the best split according to `criterion`\n",
    "        feat, thresh = self._segment(X, Y, feat_idxs)\n",
    "        l = np.argwhere(X[:, feat] <= thresh).flatten()\n",
    "        r = np.argwhere(X[:, feat] > thresh).flatten()\n",
    "\n",
    "        # grow the children that result from the split\n",
    "        left = self._grow(X[l, :], Y[l])\n",
    "        right = self._grow(X[r, :], Y[r])\n",
    "        return Node(left, right, (feat, thresh))\n",
    "\n",
    "    def _segment(self, X, Y, feat_idxs):\n",
    "        best_gain = -np.inf\n",
    "        split_idx, split_thresh = None, None\n",
    "        for i in feat_idxs:\n",
    "            vals = X[:, i]\n",
    "            levels = np.unique(vals)\n",
    "            thresholds = (levels[:-1] + levels[1:]) / 2\n",
    "            gains = np.array([self._impurity_gain(Y, t, vals) for t in thresholds])\n",
    "\n",
    "            if gains.max() > best_gain:\n",
    "                split_idx = i\n",
    "                best_gain = gains.max()\n",
    "                split_thresh = thresholds[gains.argmax()]\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _impurity_gain(self, Y, split_thresh, feat_values):\n",
    "        if self.criterion == \"entropy\":\n",
    "            loss = entropy\n",
    "        elif self.criterion == \"gini\":\n",
    "            loss = gini\n",
    "        elif self.criterion == \"mse\":\n",
    "            loss = mse\n",
    "\n",
    "        parent_loss = loss(Y)\n",
    "\n",
    "        # generate split\n",
    "        left = np.argwhere(feat_values <= split_thresh).flatten()\n",
    "        right = np.argwhere(feat_values > split_thresh).flatten()\n",
    "\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "\n",
    "        # compute the weighted avg. of the loss for the children\n",
    "        n = len(Y)\n",
    "        n_l, n_r = len(left), len(right)\n",
    "        e_l, e_r = loss(Y[left]), loss(Y[right])\n",
    "        child_loss = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # impurity gain is difference in loss before vs. after split\n",
    "        ig = parent_loss - child_loss\n",
    "        return ig\n",
    "\n",
    "    def _traverse(self, X, node, prob=False):\n",
    "        if isinstance(node, Leaf):\n",
    "            if self.classifier:\n",
    "                return node.value if prob else node.value.argmax()\n",
    "            return node.value\n",
    "        if X[node.feature] <= node.threshold:\n",
    "            return self._traverse(X, node.left, prob)\n",
    "        return self._traverse(X, node.right, prob)\n",
    "\n",
    "\n",
    "def mse(y):\n",
    "    return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "\n",
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / np.sum(hist)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    hist = np.bincount(y)\n",
    "    N = np.sum(hist)\n",
    "    return 1 - sum([(i / N) ** 2 for i in hist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#######################################################################\n",
    "#                           Base Estimators                           #\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "class ClassProbEstimator:\n",
    "    def fit(self, X, y):\n",
    "        self.class_prob = y.sum() / len(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = np.empty(X.shape[0], dtype=np.float64)\n",
    "        pred.fill(self.class_prob)\n",
    "        return pred\n",
    "\n",
    "\n",
    "class MeanBaseEstimator:\n",
    "    def fit(self, X, y):\n",
    "        self.avg = np.mean(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = np.empty(X.shape[0], dtype=np.float64)\n",
    "        pred.fill(self.avg)\n",
    "        return pred\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#                           Loss Functions                            #\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "class MSELoss:\n",
    "    def __call__(self, y, y_pred):\n",
    "        return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "    def base_estimator(self):\n",
    "        return MeanBaseEstimator()\n",
    "\n",
    "    def grad(self, y, y_pred):\n",
    "        return -2 / len(y) * (y - y_pred)\n",
    "\n",
    "    def line_search(self, y, y_pred, h_pred):\n",
    "        # TODO: revise this\n",
    "        Lp = np.sum((y - y_pred) * h_pred)\n",
    "        Lpp = np.sum(h_pred * h_pred)\n",
    "\n",
    "        # if we perfectly fit the residuals, use max step size\n",
    "        return 1 if np.sum(Lpp) == 0 else Lp / Lpp\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __call__(self, y, y_pred):\n",
    "        eps = np.finfo(float).eps\n",
    "        return -np.sum(y * np.log(y_pred + eps))\n",
    "\n",
    "    def base_estimator(self):\n",
    "        return ClassProbEstimator()\n",
    "\n",
    "    def grad(self, y, y_pred):\n",
    "        eps = np.finfo(float).eps\n",
    "        return -y * 1 / (y_pred + eps)\n",
    "\n",
    "    def line_search(self, y, y_pred, h_pred):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, n_classes=None):\n",
    "    if labels.ndim > 1:\n",
    "        raise ValueError(\"labels must have dimension 1, but got {}\".format(labels.ndim))\n",
    "\n",
    "    N = labels.size\n",
    "    n_cols = np.max(labels) + 1 if n_classes is None else n_classes\n",
    "    one_hot = np.zeros((N, n_cols))\n",
    "    one_hot[np.arange(N), labels] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "class GradientBoostedDecisionTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_iter,\n",
    "        max_depth=None,\n",
    "        classifier=True,\n",
    "        learning_rate=1,\n",
    "        loss=\"crossentropy\",\n",
    "        step_size=\"constant\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A gradient boosted ensemble of decision trees.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Gradient boosted machines (GBMs) fit an ensemble of `m` weak learners such that:\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            f_m(X) = b(X) + \\eta w_1 g_1 + \\ldots + \\eta w_m g_m\n",
    "\n",
    "        where `b` is a fixed initial estimate for the targets, :math:`\\eta` is\n",
    "        a learning rate parameter, and :math:`w_{\\cdot}` and :math:`g_{\\cdot}`\n",
    "        denote the weights and learner predictions for subsequent fits.\n",
    "\n",
    "        We fit each `w` and `g` iteratively using a greedy strategy so that at each\n",
    "        iteration `i`,\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            w_i, g_i = \\\\arg \\min_{w_i, g_i} L(Y, f_{i-1}(X) + w_i g_i)\n",
    "\n",
    "        On each iteration we fit a new weak learner to predict the negative\n",
    "        gradient of the loss with respect to the previous prediction, :math:`f_{i-1}(X)`.\n",
    "        We then use the element-wise product of the predictions of this weak\n",
    "        learner, :math:`g_i`, with a weight, :math:`w_i`, to compute the amount to\n",
    "        adjust the predictions of our model at the previous iteration, :math:`f_{i-1}(X)`:\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            f_i(X) := f_{i-1}(X) + w_i g_i\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_iter : int\n",
    "            The number of iterations / weak estimators to use when fitting each\n",
    "            dimension / class of `Y`.\n",
    "        max_depth : int\n",
    "            The maximum depth of each decision tree weak estimator. Default is\n",
    "            None.\n",
    "        classifier : bool\n",
    "            Whether `Y` contains class labels or real-valued targets. Default\n",
    "            is True.\n",
    "        learning_rate : float\n",
    "            Value in [0, 1] controlling the amount each weak estimator\n",
    "            contributes to the overall model prediction. Sometimes known as the\n",
    "            `shrinkage parameter` in the GBM literature. Default is 1.\n",
    "        loss : {'crossentropy', 'mse'}\n",
    "            The loss to optimize for the GBM. Default is 'crossentropy'.\n",
    "        step_size : {\"constant\", \"adaptive\"}\n",
    "            How to choose the weight for each weak learner. If \"constant\", use\n",
    "            a fixed weight of 1 for each learner. If \"adaptive\", use a step\n",
    "            size computed via line-search on the current iteration's loss.\n",
    "            Default is 'constant'.\n",
    "        \"\"\"\n",
    "        self.loss = loss\n",
    "        self.weights = None\n",
    "        self.learners = None\n",
    "        self.out_dims = None\n",
    "        self.n_iter = n_iter\n",
    "        self.base_estimator = None\n",
    "        self.max_depth = max_depth\n",
    "        self.step_size = step_size\n",
    "        self.classifier = classifier\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fit the gradient boosted decision trees on a dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape (N, M)\n",
    "            The training data of `N` examples, each with `M` features\n",
    "        Y : :py:class:`ndarray <numpy.ndarray>` of shape (N,)\n",
    "            An array of integer class labels for each example in `X` if\n",
    "            ``self.classifier = True``, otherwise the set of target values for\n",
    "            each example in `X`.\n",
    "        \"\"\"\n",
    "        if self.loss == \"mse\":\n",
    "            loss = MSELoss()\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            loss = CrossEntropyLoss()\n",
    "\n",
    "        # convert Y to one_hot if not already\n",
    "        if self.classifier:\n",
    "            Y = to_one_hot(Y.flatten())\n",
    "        else:\n",
    "            Y = Y.reshape(-1, 1) if len(Y.shape) == 1 else Y\n",
    "\n",
    "        N, M = X.shape\n",
    "        self.out_dims = Y.shape[1]\n",
    "        self.learners = np.empty((self.n_iter, self.out_dims), dtype=object)\n",
    "        self.weights = np.ones((self.n_iter, self.out_dims))\n",
    "        self.weights[1:, :] *= self.learning_rate\n",
    "\n",
    "        # fit the base estimator\n",
    "        Y_pred = np.zeros((N, self.out_dims))\n",
    "        for k in range(self.out_dims):\n",
    "            t = loss.base_estimator()\n",
    "            t.fit(X, Y[:, k])\n",
    "            Y_pred[:, k] += t.predict(X)\n",
    "            self.learners[0, k] = t\n",
    "\n",
    "        # incrementally fit each learner on the negative gradient of the loss\n",
    "        # wrt the previous fit (pseudo-residuals)\n",
    "        for i in range(1, self.n_iter):\n",
    "            for k in range(self.out_dims):\n",
    "                y, y_pred = Y[:, k], Y_pred[:, k]\n",
    "                neg_grad = -1 * loss.grad(y, y_pred)\n",
    "\n",
    "                # use MSE as the surrogate loss when fitting to negative gradients\n",
    "                t = DecisionTree(\n",
    "                    classifier=False, max_depth=self.max_depth, criterion=\"mse\"\n",
    "                )\n",
    "\n",
    "                # fit current learner to negative gradients\n",
    "                t.fit(X, neg_grad)\n",
    "                self.learners[i, k] = t\n",
    "\n",
    "                # compute step size and weight for the current learner\n",
    "                step = 1.0\n",
    "                h_pred = t.predict(X)\n",
    "                if self.step_size == \"adaptive\":\n",
    "                    step = loss.line_search(y, y_pred, h_pred)\n",
    "\n",
    "                # update weights and our overall prediction for Y\n",
    "                self.weights[i, k] *= step\n",
    "                Y_pred[:, k] += self.weights[i, k] * h_pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to classify or predict the examples in `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : :py:class:`ndarray <numpy.ndarray>` of shape `(N, M)`\n",
    "            The training data of `N` examples, each with `M` features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        preds : :py:class:`ndarray <numpy.ndarray>` of shape `(N,)`\n",
    "            The integer class labels predicted for each example in `X` if\n",
    "            ``self.classifier = True``, otherwise the predicted target values.\n",
    "        \"\"\"\n",
    "        Y_pred = np.zeros((X.shape[0], self.out_dims))\n",
    "        for i in range(self.n_iter):\n",
    "            for k in range(self.out_dims):\n",
    "                Y_pred[:, k] += self.weights[i, k] * self.learners[i, k].predict(X)\n",
    "\n",
    "        if self.classifier:\n",
    "            Y_pred = Y_pred.argmax(axis=1)\n",
    "\n",
    "        return Y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyter)",
   "language": "python",
   "name": "pycharm-34f9d306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
