{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna-Q\n",
    "\n",
    "从真实和模拟经验中学习和规划价值函数(或策略)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{Initialize Q(s,a) and Model(s,a) }\\forall s \\in S, a \\in A(s) \\\\\n",
    "&\\text{Do forever}\\\\\n",
    "&\\quad S \\leftarrow \\text{current (nonterminal) state}\\\\\n",
    "&\\quad A \\leftarrow \\epsilon\\,greedy(S,Q)\\\\\n",
    "&\\quad\\text{Execute action A; Observe resultant reward R and state S'}\\\\\n",
    "&\\quad Q(S, A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma\\max_aQ(S', a) - Q(S, A)]\\\\\n",
    "&\\quad Model(S, A) \\leftarrow R, S' \\text{(assuming deterministic environment)}\\\\\n",
    "&\\quad\\text{Repeat n times:}\\\\\n",
    "&\\quad\\quad S \\leftarrow \\text{random previously observed state}\\\\\n",
    "&\\quad\\quad A \\leftarrow \\text{random action previously taken in S}\\\\\n",
    "&\\quad\\quad R, S' \\leftarrow Model(S, A)\\\\\n",
    "&\\quad\\quad Q(S, A) \\leftarrow Q(S, A) + \\alpha[R + \\gamma\\max_aQ(S', a) - Q(S, A)\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化 gym 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: Discrete(6)\n",
      "observation_space: Discrete(500)\n",
      "action sample: 0\n",
      "observation sample: 315\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "np.random.seed(12345)\n",
    "\n",
    "env = gym.make('Taxi-v2')\n",
    "env.seed(seed=12345)\n",
    "\n",
    "print('action_space:', env.action_space)\n",
    "print('observation_space:', env.observation_space)\n",
    "\n",
    "print('action sample:', env.action_space.sample())\n",
    "print('observation sample:', env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 Model 类保存 state , action , reward 和 state_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class EnvModel:\n",
    "    def __init__(self, actions):\n",
    "        # the simplest case is to think about the model is a memory which has all past transition information\n",
    "        self.actions = actions\n",
    "        self.database = pd.DataFrame(columns=actions, dtype=np.object)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if s not in self.database.index:\n",
    "            self.database = self.database.append(\n",
    "                pd.Series(\n",
    "                    [None] * len(self.actions),\n",
    "                    index=self.database.columns,\n",
    "                    name=s,\n",
    "                ))\n",
    "        self.database.at[s, a] = (r, s_)\n",
    "\n",
    "    def sample_s_a(self):\n",
    "        s = np.random.choice(self.database.index)\n",
    "        a = np.random.choice(self.database.loc[s].dropna().index)    # filter out the None value\n",
    "        return s, a\n",
    "\n",
    "    def get_r_s_(self, s, a):\n",
    "        r, s_ = self.database.loc[s, a]\n",
    "        return r, s_\n",
    "    \n",
    "\n",
    "envModel = EnvModel(actions=list(range(env.action_space.n)))\n",
    "\n",
    "def init_model():\n",
    "    global envModel\n",
    "    envModel = EnvModel(actions=list(range(env.action_space.n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-Greedy 探索\n",
    "\n",
    "- 最简单的方法确保持续探索\n",
    "- 所有 m 个动作都采取非零概率\n",
    "- $1 - \\epsilon$ 概率: 选择贪婪动作\n",
    "- $\\epsilon$ 概率: 选择随机动作\n",
    "\n",
    "则:\n",
    "$$\n",
    "\\pi(a|s) = \\begin{cases} \\frac\\epsilon m + 1 - \\epsilon & (a^* == \\mathop{argmax}_{a\\in A} Q(s, a)) \\\\ \\frac\\epsilon m & (otherwise) \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(a, epsilon):\n",
    "    m = len(a)\n",
    "    assert m > 0, 'a length must large then 0'\n",
    "    p_a = epsilon / m\n",
    "    p_a_star = 1. - epsilon + p_a\n",
    "    action_probs = np.ones(m) * p_a\n",
    "    action_probs[np.argmax(a)] = p_a_star\n",
    "    np.testing.assert_allclose(np.sum(action_probs), 1)\n",
    "    act_num = np.random.multinomial(1, action_probs).argmax()\n",
    "    return act_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "q_s_a = np.random.rand(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "def init_q_value():\n",
    "    global q_s_a\n",
    "    q_s_a = np.random.rand(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "print(q_s_a.shape)\n",
    "print(epsilon_greedy_policy(q_s_a[0], 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作选择:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(obs, greedy = False):\n",
    "    return epsilon_greedy_policy(q_s_a[obs], 0.1) if not greedy else np.argmax(q_s_a[obs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更新 q_value\n",
    "\n",
    "$$\n",
    "Q(S, A) \\leftarrow Q(S,A) + \\alpha[R + \\gamma\\max_aQ(S', a) - Q(S, A)]\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_value(s, a, alpha, gamma, reward, s_next):\n",
    "    global q_s_a\n",
    "    q_s_a[s][a] = q_s_a[s][a] + alpha * (reward + gamma * (np.max(q_s_a[s_next]) if s_next else 0) - q_s_a[s][a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 每个 episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_step, n_times):\n",
    "    s = env.reset()\n",
    "    for i in range(max_step):\n",
    "        a = choose_action(s)\n",
    "        s_, reward, done, info = env.step(a)\n",
    "        \n",
    "        s_next = s_ if not done else None\n",
    "        update_q_value(s, a, 0.1, 0.9, reward, s_next)\n",
    "        envModel.store_transition(s, a, reward, s_next)\n",
    "        \n",
    "        for n in range(n_times):\n",
    "            s_sim, a_sim = envModel.sample_s_a()\n",
    "            r_sim, s_next_sim = envModel.get_r_s_(s_sim, a_sim)\n",
    "            update_q_value(s_sim, a_sim, 0.1, 0.9, r_sim, s_next_sim)\n",
    "        \n",
    "        s = s_\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74869652 0.15671029 0.22323155 0.47084724 0.81668545 0.61839826]\n",
      " [0.20904312 0.99214169 0.54175797 0.5814712  0.93375944 0.13073146]\n",
      " [0.67290178 0.7416259  0.7656646  0.44903566 0.70600245 0.30681361]\n",
      " ...\n",
      " [0.27952323 0.26565864 0.90590145 0.57269107 0.53959644 0.07835126]\n",
      " [0.22998933 0.1997635  0.58645426 0.36092565 0.57610072 0.35979278]\n",
      " [0.77372826 0.9912789  0.33209604 0.64691804 0.28111215 0.8573636 ]]\n",
      "[[ 0.74869652  0.15671029  0.22323155  0.47084724  0.81668545  0.61839826]\n",
      " [ 0.20904312  0.20283594  0.32677129  0.47565649 -0.04842492  0.13073146]\n",
      " [ 0.67290178  0.7416259   0.7656646   0.44903566  0.70600245  0.30681361]\n",
      " ...\n",
      " [ 0.27952323  0.26565864  0.90590145  0.57269107  0.53959644  0.07835126]\n",
      " [ 0.22998933  0.1997635   0.58645426  0.36092565  0.57610072  0.35979278]\n",
      " [ 0.77372826  0.9912789   0.33209604  0.64691804  0.28111215  0.8573636 ]]\n"
     ]
    }
   ],
   "source": [
    "init_model()\n",
    "init_q_value()\n",
    "print(q_s_a)\n",
    "run_episode(100, 10)\n",
    "print(q_s_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(times):\n",
    "    init_model()\n",
    "    init_q_value()\n",
    "    for i in range(times):\n",
    "        run_episode(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(max_step):\n",
    "    env.reset()\n",
    "    rewards = 0\n",
    "    for i in range(max_step):\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        rewards = rewards + reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    print('random action', rewards)\n",
    "    \n",
    "    obs = env.reset()\n",
    "    rewards = 0\n",
    "    for i in range(max_step):\n",
    "        obs, reward, done, info = env.step(choose_action(obs, True))\n",
    "        rewards = rewards + reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    print('dyna q action', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random action -388\n",
      "dyna q action -100\n"
     ]
    }
   ],
   "source": [
    "test(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyter)",
   "language": "python",
   "name": "pycharm-34f9d306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
