{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo\n",
    "\n",
    "创建 gym 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space Tuple(Discrete(2), Discrete(2), Discrete(5))\n",
      "observation_space Discrete(6)\n",
      "(1, 1, 3)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "np.random.seed(12345)\n",
    "\n",
    "env = gym.make('Copy-v0')\n",
    "env.seed(seed=12345)\n",
    "\n",
    "print('action_space', env.action_space)\n",
    "print('observation_space', env.observation_space)\n",
    "\n",
    "print(env.action_space.sample())\n",
    "print(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的 action_space 是一个 3 维的离散值,这里需要做一个转换:\n",
    "\n",
    "(0,0,0) <=> 0   \n",
    "(0,0,1) <=> 1   \n",
    "...   \n",
    "(1, 1, 4) <=> 19   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "action_list = list(product(*[range(i) for i in [act_space.n for act_space in env.action_space]]))\n",
    "def num2action(num):\n",
    "    return action_list[num]\n",
    "\n",
    "num_action_dict = {act: i for i, act in enumerate(action_list)}\n",
    "def action2num(action):\n",
    "    return num_action_dict[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-Greedy 探索\n",
    "\n",
    "- 最简单的方法确保持续探索\n",
    "- 所有 m 个动作都采取非零概率\n",
    "- $1 - \\epsilon$ 概率: 选择贪婪动作\n",
    "- $\\epsilon$ 概率: 选择随机动作\n",
    "\n",
    "则:\n",
    "$$\n",
    "\\pi(a|s) = \\begin{cases} \\frac\\epsilon m + 1 - \\epsilon & (a^* == \\mathop{argmax}_{a\\in A} Q(s, a)) \\\\ \\frac\\epsilon m & (otherwise) \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(a, epsilon):\n",
    "    m = len(a)\n",
    "    assert m > 0, 'a length must large then 0'\n",
    "    p_a = epsilon / m\n",
    "    p_a_star = 1. - epsilon + p_a\n",
    "    action_probs = np.ones(m) * p_a\n",
    "    action_probs[a.argmax()] = p_a_star\n",
    "    np.testing.assert_allclose(np.sum(action_probs), 1)\n",
    "    act_num = np.random.multinomial(1, action_probs).argmax()\n",
    "    return num2action(act_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 20)\n",
      "(1, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "q_s_a = np.random.rand(env.observation_space.n, np.prod([act_space.n for act_space in env.action_space]))\n",
    "print(q_s_a.shape)\n",
    "print(epsilon_greedy_policy(q_s_a[0], 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    return epsilon_greedy_policy(q_s_a[state], 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 $\\epsilon$-Greedy 完成 1 个 episode:\n",
    "\n",
    "  $$\n",
    "  \\{S_1, A_1, R_2, ..., S_T\\} \\sim \\pi\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_step):\n",
    "    history_s_a = []\n",
    "    history_reward = []\n",
    "    \n",
    "    s = env.reset()\n",
    "    for i in range(max_step):\n",
    "        action = choose_action(s)\n",
    "        history_s_a.append((s, action2num(action)))\n",
    "        \n",
    "        s, reward, done, info = env.step(action)\n",
    "        history_reward.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return history_s_a, history_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(2, 13), (2, 13), (2, 13), (2, 1), (2, 13), (2, 13), (5, 15)], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5])\n"
     ]
    }
   ],
   "source": [
    "print(run_episode(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 每个 episode 中的状态 $S_t$ 和动作 $A_t$:\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  & N(S_t, A_t) = N(S_t, A_t) + 1 \\\\\n",
    "  & Q(S_t, A_t) = Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)}(G_t - Q(S_t, A_t))\n",
    "  \\end{align*}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_value(history_s_a, history_reward, n_s_a):\n",
    "    set_s_a = set(history_s_a)\n",
    "    g_t = [np.sum(history_reward[i:]) for i in [history_s_a.index(sa) for sa in set_s_a]]\n",
    "    for i, (s, a) in enumerate(set_s_a):\n",
    "        n_s_a[s][a] = n_s_a[s][a] + 1\n",
    "        q_s_a[s][a] = q_s_a[s][a] + 1 / n_s_a[s][a] * (g_t[i] - q_s_a[s][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_s_a = np.zeros((env.observation_space.n, np.prod([act_space.n for act_space in env.action_space])))\n",
    "history_s_a, history_reward = run_episode(100)\n",
    "update_q_value(history_s_a, history_reward, n_s_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练 10000 个 episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.800e+01 6.100e+01 5.700e+01 6.200e+01 5.400e+01 1.260e+02 7.100e+01\n",
      "  4.500e+01 6.200e+01 6.200e+01 7.100e+01 7.300e+01 5.200e+01 8.000e+01\n",
      "  4.900e+01 6.279e+03 5.700e+01 5.100e+01 5.900e+01 6.900e+01]\n",
      " [6.800e+01 6.400e+01 5.900e+01 7.000e+01 6.000e+01 6.200e+01 4.800e+01\n",
      "  7.200e+01 5.800e+01 6.000e+01 6.200e+01 7.400e+01 5.100e+01 7.400e+01\n",
      "  5.200e+01 6.300e+01 6.286e+03 8.000e+01 5.500e+01 7.000e+01]\n",
      " [6.900e+01 7.000e+01 8.400e+01 6.100e+01 6.100e+01 7.000e+01 7.500e+01\n",
      "  1.350e+02 6.400e+01 6.900e+01 6.200e+01 6.100e+01 5.800e+01 5.700e+01\n",
      "  5.900e+01 4.600e+01 7.500e+01 6.203e+03 4.700e+01 5.100e+01]\n",
      " [6.700e+01 6.900e+01 5.800e+01 5.400e+01 7.000e+01 5.400e+01 6.200e+01\n",
      "  6.100e+01 8.600e+01 5.800e+01 5.500e+01 5.200e+01 7.200e+01 5.300e+01\n",
      "  6.200e+01 5.400e+01 6.900e+01 6.300e+01 6.224e+03 5.400e+01]\n",
      " [5.400e+01 7.600e+01 6.600e+01 5.800e+01 6.700e+01 6.500e+01 6.400e+01\n",
      "  6.400e+01 6.300e+01 7.300e+01 5.400e+01 6.900e+01 6.100e+01 6.800e+01\n",
      "  4.600e+01 5.500e+01 6.200e+01 6.200e+01 6.300e+01 6.162e+03]\n",
      " [1.300e+01 1.100e+01 6.000e+00 9.000e+00 1.100e+01 1.300e+01 8.000e+00\n",
      "  1.400e+01 7.000e+00 2.600e+01 5.000e+00 1.200e+01 1.000e+01 3.910e+02\n",
      "  8.000e+00 9.000e+00 8.000e+00 1.100e+01 8.000e+00 1.140e+02]]\n",
      "[[ 1.27941176  0.16393443  0.97368421  0.44354839  0.50925926  0.78571429\n",
      "  -0.45070423 -0.47777778 -0.48387097 -0.5        -0.21126761 -0.26027397\n",
      "  -0.18269231 -0.38125     0.10204082  4.4198917  -0.48245614 -0.5\n",
      "  -0.46610169 -0.48550725]\n",
      " [ 0.89705882  0.5859375   0.61016949  1.00714286  0.46666667 -0.5\n",
      "   0.57291667 -0.48611111 -0.5        -0.5        -0.03225806  0.07432432\n",
      "  -0.24509804 -0.0472973  -0.09615385 -0.5         4.42602609 -0.4875\n",
      "  -0.46363636 -0.47857143]\n",
      " [ 0.24637681  0.67857143  0.83333333  0.59836066  1.02459016 -0.5\n",
      "  -0.46        0.81851852 -0.46875    -0.5         0.10483871  0.07377049\n",
      "  -0.18965517 -0.09649123 -0.43220339 -0.5        -0.48666667  4.43704659\n",
      "  -0.4787234  -0.5       ]\n",
      " [ 1.2761194   0.57246377  0.01724138  0.17592593  1.31428571 -0.42592593\n",
      "  -0.5        -0.40983607  0.76162791 -0.48275862  0.02727273 -0.02884615\n",
      "  -0.29861111 -0.28301887 -0.16129032 -0.5        -0.48550725 -0.5\n",
      "   4.4624036  -0.5       ]\n",
      " [ 1.12037037  1.02631579  0.07575758  0.51724138  0.70149254 -0.5\n",
      "  -0.5        -0.5        -0.48412698  0.69178082 -0.25       -0.10144928\n",
      "   0.12295082 -0.17647059 -0.17391304 -0.5        -0.46774194 -0.46774194\n",
      "  -0.46825397  4.42031808]\n",
      " [ 0.38461538  0.36363636  2.         -0.27777778 -0.81818182 -0.38461538\n",
      "  -0.4375     -0.03571429 -0.35714286 -0.25       -0.6         0.33333333\n",
      "   0.65        2.68414322 -0.875      -0.16666667 -0.1875     -0.45454545\n",
      "  -0.3125     -0.15789474]]\n"
     ]
    }
   ],
   "source": [
    "n_s_a = np.zeros((env.observation_space.n, np.prod([act_space.n for act_space in env.action_space])))\n",
    "for i in range(10000):\n",
    "    history_s_a, history_reward = run_episode(100)\n",
    "    update_q_value(history_s_a, history_reward, n_s_a)\n",
    "    \n",
    "print(n_s_a)\n",
    "print(q_s_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(2, 17), (2, 17), (4, 19), (1, 16), (0, 15), (0, 15), (4, 19), (4, 19)], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n"
     ]
    }
   ],
   "source": [
    "print(run_episode(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyter)",
   "language": "python",
   "name": "pycharm-34f9d306"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
